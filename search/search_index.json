{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is this? \u00b6 Freenalytics is an open source usage analytics platform for any type of application. It serves as a tool to empower small development teams that need a customizable analytics compilation service in their applications that can be self-hosted easily. Why? \u00b6 This project aims to be a simple yet powerful analytics dashboard for any type of application that can benefit from usage analytics, from simple websites, to server applications, to maybe even bots for Discord . It is still in an early stage and currently works as a \"simple log aggregator\" with bare bones visualization tools. Finally, it was built as a final college project as a requirement to get my Computer Science Engineer degree. Got any Screenshots? \u00b6 In case you're curious to see what this application looks like, take a look at the next screenshots. Applications page Create your application Web application dashboard Discord Bot application dashboard Raw data entries Information page to guide you on how to upload your data Interested? \u00b6 Check out the Getting Started page to start using Freenalytics today. Author \u00b6 This application was made by moonstar-x for Freenalytics .","title":"Home"},{"location":"#what-is-this","text":"Freenalytics is an open source usage analytics platform for any type of application. It serves as a tool to empower small development teams that need a customizable analytics compilation service in their applications that can be self-hosted easily.","title":"What is this?"},{"location":"#why","text":"This project aims to be a simple yet powerful analytics dashboard for any type of application that can benefit from usage analytics, from simple websites, to server applications, to maybe even bots for Discord . It is still in an early stage and currently works as a \"simple log aggregator\" with bare bones visualization tools. Finally, it was built as a final college project as a requirement to get my Computer Science Engineer degree.","title":"Why?"},{"location":"#got-any-screenshots","text":"In case you're curious to see what this application looks like, take a look at the next screenshots. Applications page Create your application Web application dashboard Discord Bot application dashboard Raw data entries Information page to guide you on how to upload your data","title":"Got any Screenshots?"},{"location":"#interested","text":"Check out the Getting Started page to start using Freenalytics today.","title":"Interested?"},{"location":"#author","text":"This application was made by moonstar-x for Freenalytics .","title":"Author"},{"location":"data-treatment/","text":"Data Treatment \u00b6 This section contains some examples of what types of treatment you can give your own data through the Export as CSV functionality. Some of these examples include: Page Clicks View","title":"Introduction"},{"location":"data-treatment/#data-treatment","text":"This section contains some examples of what types of treatment you can give your own data through the Export as CSV functionality. Some of these examples include: Page Clicks View","title":"Data Treatment"},{"location":"data-treatment/page-clicks-view/","text":"Page Clicks View \u00b6 The script tools-page-clicks-view is a little script that can generate a screenshot of a website and add some circles of where the users have clicked. The script was made with the Official Web Template in mind and it is by no means a generic solution since it was made for the Webpage Example . A guide on how to update the script is inside the repository's README . Implementation \u00b6 The implementation logic is pretty simple and it goes something like this: Taking a Screenshot \u00b6 We can use something like Selenium to get a screenshot of the webpage. def screenshot_page ( url : str , screenshot_destination : str , ** kwargs ): force_width = kwargs . get ( 'width' ) force_height = kwargs . get ( 'height' ) driver = webdriver . Chrome ( options = options ) driver . get ( url ) width , height = get_page_dimensions ( driver ) driver . set_window_size ( force_width or width , force_height or height ) driver . find_element ( By . TAG_NAME , 'body' ) . screenshot ( screenshot_destination ) driver . quit () Get the Page Click Coordinates \u00b6 We then parse the exported CSV to get all the clicks on a specific page, we get the coordinates and then normalize them with the user's page size. def get_page_click_normal_coordinates ( csv_data : str , route_filter : str ) -> np . array : df = pd . read_csv ( csv_data ) filtered_df = df . loc [ df [ 'element_clicked.url_route' ] == route_filter ] page_x = filtered_df [ 'element_clicked.page_x' ] . to_numpy () page_width = filtered_df [ 'element_clicked.page_width' ] . to_numpy () page_y = filtered_df [ 'element_clicked.page_y' ] . to_numpy () page_height = filtered_df [ 'element_clicked.page_height' ] . to_numpy () normal_x = page_x / page_width normal_y = page_y / page_height return np . array ( list ( zip ( normal_x , normal_y ))) Apply Circles Where Users Have Clicked \u00b6 We can then treat the screenshot taken previously and add circles where all the users have clicked on the page: def overlay_page_clicks ( raw_screenshot_path : str , normal_coordinates : np . array , output_path : str , ** kwargs ): radius = kwargs . get ( 'radius' ) or DEFAULT_CIRCLE_RADIUS color = kwargs . get ( 'color' ) or DEFAULT_COLOR alpha = kwargs . get ( 'alpha' ) or DEFAULT_ALPHA img = plt . imread ( raw_screenshot_path ) img_width = img . shape [ 1 ] img_height = img . shape [ 0 ] px = 1 / plt . rcParams [ 'figure.dpi' ] fig , ax = plt . subplots ( figsize = ( img_width * px , img_height * px )) ax . set_aspect ( 'equal' ) for page_x , page_y in normal_coordinates : x = page_x * img_width y = page_y * img_height circle = Circle (( x , y ), radius , facecolor = color , alpha = alpha ) ax . add_patch ( circle ) ax . imshow ( img ) plt . axis ( 'off' ) plt . savefig ( output_path , bbox_inches = 'tight' , pad_inches = 0 ) And that's it. We have now generated images to see where users have clicked. Further improvement of this script could include: Make it generic and automatize it. Instead of using circles to display user clicks, draw a heatmap over the page screenshot. Results \u00b6 Check out the results of running this script: Example Home page. Example article page. Example about page.","title":"Page Clicks View"},{"location":"data-treatment/page-clicks-view/#page-clicks-view","text":"The script tools-page-clicks-view is a little script that can generate a screenshot of a website and add some circles of where the users have clicked. The script was made with the Official Web Template in mind and it is by no means a generic solution since it was made for the Webpage Example . A guide on how to update the script is inside the repository's README .","title":"Page Clicks View"},{"location":"data-treatment/page-clicks-view/#implementation","text":"The implementation logic is pretty simple and it goes something like this:","title":"Implementation"},{"location":"data-treatment/page-clicks-view/#taking-a-screenshot","text":"We can use something like Selenium to get a screenshot of the webpage. def screenshot_page ( url : str , screenshot_destination : str , ** kwargs ): force_width = kwargs . get ( 'width' ) force_height = kwargs . get ( 'height' ) driver = webdriver . Chrome ( options = options ) driver . get ( url ) width , height = get_page_dimensions ( driver ) driver . set_window_size ( force_width or width , force_height or height ) driver . find_element ( By . TAG_NAME , 'body' ) . screenshot ( screenshot_destination ) driver . quit ()","title":"Taking a Screenshot"},{"location":"data-treatment/page-clicks-view/#get-the-page-click-coordinates","text":"We then parse the exported CSV to get all the clicks on a specific page, we get the coordinates and then normalize them with the user's page size. def get_page_click_normal_coordinates ( csv_data : str , route_filter : str ) -> np . array : df = pd . read_csv ( csv_data ) filtered_df = df . loc [ df [ 'element_clicked.url_route' ] == route_filter ] page_x = filtered_df [ 'element_clicked.page_x' ] . to_numpy () page_width = filtered_df [ 'element_clicked.page_width' ] . to_numpy () page_y = filtered_df [ 'element_clicked.page_y' ] . to_numpy () page_height = filtered_df [ 'element_clicked.page_height' ] . to_numpy () normal_x = page_x / page_width normal_y = page_y / page_height return np . array ( list ( zip ( normal_x , normal_y )))","title":"Get the Page Click Coordinates"},{"location":"data-treatment/page-clicks-view/#apply-circles-where-users-have-clicked","text":"We can then treat the screenshot taken previously and add circles where all the users have clicked on the page: def overlay_page_clicks ( raw_screenshot_path : str , normal_coordinates : np . array , output_path : str , ** kwargs ): radius = kwargs . get ( 'radius' ) or DEFAULT_CIRCLE_RADIUS color = kwargs . get ( 'color' ) or DEFAULT_COLOR alpha = kwargs . get ( 'alpha' ) or DEFAULT_ALPHA img = plt . imread ( raw_screenshot_path ) img_width = img . shape [ 1 ] img_height = img . shape [ 0 ] px = 1 / plt . rcParams [ 'figure.dpi' ] fig , ax = plt . subplots ( figsize = ( img_width * px , img_height * px )) ax . set_aspect ( 'equal' ) for page_x , page_y in normal_coordinates : x = page_x * img_width y = page_y * img_height circle = Circle (( x , y ), radius , facecolor = color , alpha = alpha ) ax . add_patch ( circle ) ax . imshow ( img ) plt . axis ( 'off' ) plt . savefig ( output_path , bbox_inches = 'tight' , pad_inches = 0 ) And that's it. We have now generated images to see where users have clicked. Further improvement of this script could include: Make it generic and automatize it. Instead of using circles to display user clicks, draw a heatmap over the page screenshot.","title":"Apply Circles Where Users Have Clicked"},{"location":"data-treatment/page-clicks-view/#results","text":"Check out the results of running this script: Example Home page. Example article page. Example about page.","title":"Results"},{"location":"development/","text":"Development \u00b6 This section contains some information regarding the development process of this project. If you wish to contribute to the development of Freenalytics, check out the following pages: Server Development Web Dashboard Development Connector Development","title":"Introduction"},{"location":"development/#development","text":"This section contains some information regarding the development process of this project. If you wish to contribute to the development of Freenalytics, check out the following pages: Server Development Web Dashboard Development Connector Development","title":"Development"},{"location":"development/server/","text":"Server Development \u00b6 The freenalytics/freenalytics repository is a monorepo containing both the web dashboard and the server. Any changes made to the server should be made in this repository. Requirements \u00b6 In order to develop for the server you will need: git Node.js (Version 16.15.1 was used) Docker (With docker-compose ) Setting Up \u00b6 First, clone the repository: git clone https://github.com/freenalytics/freenalytics Head over to the server folder: cd server And then, install the dependencies: npm install Setting Up the Development Environment \u00b6 The server requires MongoDB and Redis instances in order to run. The server folder includes a dev folder with a docker-compose.yml file inside with a MongoDB and Redis service. Head over to this folder and run: docker-compose up This will start both a MongoDB server at port 27017 and a Redis server at port 6379 . Environment Variables \u00b6 Inside the server folder create a .env file with the following content: MONGODB_URI=mongodb://root:password@localhost:27017/freenalytics?authSource=admin REDIS_URI=redis://localhost:6379 JWT_SECRET=my_super_secret JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true Note Nothing in this folder needs to be updated. It already sets the MongoDB and Redis URIs to the appropriate values, taking into account that you have started these services with the docker-compose.yml file that was previously mentioned. Starting the Server \u00b6 Development Mode \u00b6 There are two ways to start the server in development mode: If you want to run the server without automatically restarting on file save you can use: npm run dev If you wish to use watch mode (server restarts on file save), use: npm run dev:watch In both cases the server will start on port 4000 . Production Mode \u00b6 In case you wish to start the server in production mode: You need to first build the server: npm run build And then start it: npm run start Manually Testing \u00b6 In order to check functionality while developing the server, you may need an HTTP client such as Insomnia or Postman . The URL to test will be: http://localhost:4000/api . Check the src/routes folder for the relevant routers that you wish to test. Considerations \u00b6 Linting \u00b6 This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix Unit Testing \u00b6 This project contains unit tests for every component. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch Developing Routes \u00b6 Route development will usually go like this: You create a service function to query or mutate the Mongo database. You create a controller function that will handle the incoming request. You create a route function that will map the controller to a specific route and HTTP method. Each of these entities are independent to ensure modularity and ease of testing. Each of these functions should be unit tested. Routers do not need to be tested because the logic inside of them is already tested in the unit test for the controller. Services that talk to the database are located inside the src/services folder. Controllers that handle requests are located inside the src/controllers folder. Routers that map the request handlers to the HTTP route and method are located in src/routes folder. Example \u00b6 Let's see the example of the data upload route which is the most complex one currently in the application. This route includes schema fetching from the Redis cache, body validation against the stored schema, and a Mongo database insertion. Model \u00b6 Inside the src/models folder you will find mongoose models that will be used to define the shape of the data stored in MongoDB. These models are used to interact with the database. // src/models/data.ts import { Schema , model } from 'mongoose' ; export interface DataModel { payload : object domain : string createdAt : Date } const dataSchema = new Schema < DataModel > ({ payload : { type : Schema . Types . Mixed , required : true }, domain : { type : String , required : true } }, { timestamps : { createdAt : 'createdAt' , updatedAt : false } }); export default model < DataModel > ( 'Data' , dataSchema ); In here, the Data model defines the shape that the data entry will have and exports the model. Service \u00b6 Next, we create a service function that will get the application schema from the Redis cache or from the Mongo database in case it does not exist in the database. This function will also set the schema inside the cache for a faster fetching the next time the schema is needed. // src/services/dataService.ts export const getApplicationSchema = async ( domain : string ) : Promise < object > => { const key = ` ${ domain } :schema` ; const cacheHit = await redisClient . exists ( key ); if ( cacheHit ) { const cachedSchema = await redisClient . get ( key ); return JSON . parse ( cachedSchema ! ); } const application = await Application . findOne ({ domain }). exec (); if ( ! application ) { throw new ResourceNotFoundError ( `Application ${ domain } was not found.` ); } await redisClient . set ( key , JSON . stringify ( application . template . schema )); return application . template . schema ; }; Next, we create a service function that will insert the data payload inside the Mongo database. // src/services/dataService.ts export const createDataForApplication = async ( domain : string , validData : object ) : Promise < DataModel > => { const data = { domain , payload : validData } as DataModel ; await new Data ( data ). save (); return data ; }; Controller \u00b6 We can now create a controller function that will handle all the logic of the route handler. // src/controllers/dataController.ts export const create = async ( req : Request , res : Response , next : NextFunction ) => { const { domain } = req . params ; try { const schema = await getApplicationSchema ( domain ); validateDataWithTemplate ( req . body , schema ); const data = await createDataForApplication ( domain , req . body ); const response = new ResponseBuilder () . withStatusCode ( HttpStatus . CREATED ) . withData ( data ); res . status ( response . statusCode ). send ( response . build ()); } catch ( error ) { next ( error ); } }; All the logic is here. We fetch the schema for the current application, we validate that the request body conforms to the fetched schema, then we insert the data into the Mongo database and finally we respond to the requesting party with the data that was inserted. Route \u00b6 Finally, we register this controller in the appropriate router with the corresponding route and method. // src/routes/applicationRouter.ts router . route ( '/:domain/data' ) . get ( verifyUser , dataController . get ) . post ( jsonBodyRequired , dataController . create ) // This is the line that has been added in this case. . all ( onlySupportedMethods ( 'GET' , 'POST' )); And that's it. Since this route was defined inside the applicationRouter.ts file, the endpoint for this route will be: POST http://localhost:4000/api/applications/:domain/data Creating Documentation \u00b6 The server has a documentation site which includes information of all the routes exposed in the API. Inside the server folder there is a documentation folder, which includes a script that generates the OpenAPI specification file that will contain all the data to render the documentation site. In order to create a new entry, you should: Inside the documentation/schemas folder, open the file corresponding to the router where you have created your route handler. Create a new RequestSchema (if your route requires a request body) and a ResponseSchema that contains the structure of the data returned by the route handler. These schemas are joi schemas and in this particular use case they only serve as a way to represent the requests and responses, and are not used for any sort of validation. Inside the documentation/routes folder, open the folder corresponding to the route of your created route handler and create a new file with the same name as the service that your route handler executes. The idea is that these files should be descriptive of what route exactly is it that they document. Inside this newly created file, export a RouteData object with all the information corresponding to your route. Inside the documentation/documentation.ts file, import your newly created route data file and include it inside the exported object in the paths object inside the corresponding object (depending on the name of the folder that you created your route data file in). These entries are in the same order that will be displayed in the documentation site, so keep that in mind. RouteData \u00b6 Your route data file should export an object of type RouteData which includes the following fields: path : The path of the route handler. ( Required string ) method : The method of the route handler. ( Required One of: get , put , post , delete , patch ) summary : A little summary of what the route handler does. ( Required string ) description : A more detailed description of what the route handler does. ( Required string ) throws : An array of the instances of the potential HttpError errors that your route handler may throw. ( Optional HttpError[] ) success : An object that describes the information of a successful response. ( Required ) success.code : The HTTP response code that your route handler responds with on successful response. ( Required number ) success.schema : The name of the ResponseSchema that your route handler responds with (if any). ( Optional string ) success.isArray : Whether the response object is an array of ResponseSchema objects or not. ( Optional boolean - Default: false ) success.binaryType : The MIME-type of the response in case the response is a binary blob. ( Optional string ) pathParams : An array of objects that describe each path parameter inside the route. ( Optional ) pathParams[].name : The name of the path parameter. ( Required string ) pathParams[].description : A description of what the path parameter represents. ( Required string ) pathParams[].type : The type of the path parameter. ( Required One of: string , number ) queryParams : An array of objects that describe each query parameter that can be used in the route. ( Optional ) queryParams[].name : The name of the query parameter. ( Required string ) queryParams[].description : A description of what the query parameter represents. ( Required string ) queryParams[].required : Whether the query parameter is required or not. ( Required boolean ) queryParams[].type : The type of the query parameter. ( Required One of: string , boolean , number ) queryParams[].isArray : Whether the query parameter is an array or not. ( Required boolean ) bodySchema : The name of the RequestSchema that your route handler needs as a JSON body (if any). ( Optional string ) tokenRequired : Whether the route handler requires a bearer token to be specified in the Authorization header. ( Optional boolean ) Note If your route has tokenRequired set to true, you need to add a UnauthorizedError instance inside throws . Example Documentation \u00b6 Here we'll use the same example as used above, for the route that handles data upload. Schema \u00b6 The relevant schemas are as following: // documentation/schemas/application.ts export const ApplicationDataRequestSchema = Joi . object ({ example : Joi.string (). required () }); export const ApplicationDataResponseSchema = Joi . object ({ domain : Joi.string (). required (), payload : ApplicationDataRequestSchema }); In this case, the example is really only an example. Since data can come in any shape or form, an example body was necessary. RouteData File \u00b6 The file with the information of this route will then be: // documentation/routes/applications/createApplicationData.ts import HttpStatus from 'http-status-codes' ; import { RouteData } from '../../types' ; import { UnauthorizedRequestError , SchemaValidationError , ResourceNotFoundError } from '../../../src/errors/http' ; const data : RouteData = { path : '/applications/:domain/data' , method : 'post' , summary : 'Create a new data entry for an application for the requesting user.' , description : `Create a new data entry for an application for the requesting user. The shape of the request body will depend on the schema that was used to create the application.` , throws : [ new UnauthorizedRequestError (), new SchemaValidationError ( '' ), new ResourceNotFoundError ( '' ) ], success : { code : HttpStatus.CREATED , schema : 'ApplicationDataResponseSchema' }, pathParams : [ { name : 'domain' , description : 'The domain of the application to upload the data to.' , type : 'string' } ], bodySchema : 'ApplicationDataRequestSchema' , tokenRequired : true }; export default data ; Including the RouteData File \u00b6 Finally, we need to include this route data file into the documentation/documentation.ts file. Just import this file and add it into the exported object. // documentation/documentation.ts import createApplicationData from './routes/applications/createApplicationData' ; const documentationData = { metadata , info , paths : { Application : { getAllApplicationsForUser , createApplicationForUser , getApplicationByDomain , updateApplicationByDomain , deleteApplicationByDomain , createApplicationData , getApplicationData , getApplicationDataAsCsv } } }; export default documentationData ; The rest of the file was omitted as it is not relevant. Here you can see that createApplicationData is exported near the end of the paths.Application object.","title":"Server"},{"location":"development/server/#server-development","text":"The freenalytics/freenalytics repository is a monorepo containing both the web dashboard and the server. Any changes made to the server should be made in this repository.","title":"Server Development"},{"location":"development/server/#requirements","text":"In order to develop for the server you will need: git Node.js (Version 16.15.1 was used) Docker (With docker-compose )","title":"Requirements"},{"location":"development/server/#setting-up","text":"First, clone the repository: git clone https://github.com/freenalytics/freenalytics Head over to the server folder: cd server And then, install the dependencies: npm install","title":"Setting Up"},{"location":"development/server/#setting-up-the-development-environment","text":"The server requires MongoDB and Redis instances in order to run. The server folder includes a dev folder with a docker-compose.yml file inside with a MongoDB and Redis service. Head over to this folder and run: docker-compose up This will start both a MongoDB server at port 27017 and a Redis server at port 6379 .","title":"Setting Up the Development Environment"},{"location":"development/server/#environment-variables","text":"Inside the server folder create a .env file with the following content: MONGODB_URI=mongodb://root:password@localhost:27017/freenalytics?authSource=admin REDIS_URI=redis://localhost:6379 JWT_SECRET=my_super_secret JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true Note Nothing in this folder needs to be updated. It already sets the MongoDB and Redis URIs to the appropriate values, taking into account that you have started these services with the docker-compose.yml file that was previously mentioned.","title":"Environment Variables"},{"location":"development/server/#starting-the-server","text":"","title":"Starting the Server"},{"location":"development/server/#development-mode","text":"There are two ways to start the server in development mode: If you want to run the server without automatically restarting on file save you can use: npm run dev If you wish to use watch mode (server restarts on file save), use: npm run dev:watch In both cases the server will start on port 4000 .","title":"Development Mode"},{"location":"development/server/#production-mode","text":"In case you wish to start the server in production mode: You need to first build the server: npm run build And then start it: npm run start","title":"Production Mode"},{"location":"development/server/#manually-testing","text":"In order to check functionality while developing the server, you may need an HTTP client such as Insomnia or Postman . The URL to test will be: http://localhost:4000/api . Check the src/routes folder for the relevant routers that you wish to test.","title":"Manually Testing"},{"location":"development/server/#considerations","text":"","title":"Considerations"},{"location":"development/server/#linting","text":"This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix","title":"Linting"},{"location":"development/server/#unit-testing","text":"This project contains unit tests for every component. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch","title":"Unit Testing"},{"location":"development/server/#developing-routes","text":"Route development will usually go like this: You create a service function to query or mutate the Mongo database. You create a controller function that will handle the incoming request. You create a route function that will map the controller to a specific route and HTTP method. Each of these entities are independent to ensure modularity and ease of testing. Each of these functions should be unit tested. Routers do not need to be tested because the logic inside of them is already tested in the unit test for the controller. Services that talk to the database are located inside the src/services folder. Controllers that handle requests are located inside the src/controllers folder. Routers that map the request handlers to the HTTP route and method are located in src/routes folder.","title":"Developing Routes"},{"location":"development/server/#example","text":"Let's see the example of the data upload route which is the most complex one currently in the application. This route includes schema fetching from the Redis cache, body validation against the stored schema, and a Mongo database insertion.","title":"Example"},{"location":"development/server/#model","text":"Inside the src/models folder you will find mongoose models that will be used to define the shape of the data stored in MongoDB. These models are used to interact with the database. // src/models/data.ts import { Schema , model } from 'mongoose' ; export interface DataModel { payload : object domain : string createdAt : Date } const dataSchema = new Schema < DataModel > ({ payload : { type : Schema . Types . Mixed , required : true }, domain : { type : String , required : true } }, { timestamps : { createdAt : 'createdAt' , updatedAt : false } }); export default model < DataModel > ( 'Data' , dataSchema ); In here, the Data model defines the shape that the data entry will have and exports the model.","title":"Model"},{"location":"development/server/#service","text":"Next, we create a service function that will get the application schema from the Redis cache or from the Mongo database in case it does not exist in the database. This function will also set the schema inside the cache for a faster fetching the next time the schema is needed. // src/services/dataService.ts export const getApplicationSchema = async ( domain : string ) : Promise < object > => { const key = ` ${ domain } :schema` ; const cacheHit = await redisClient . exists ( key ); if ( cacheHit ) { const cachedSchema = await redisClient . get ( key ); return JSON . parse ( cachedSchema ! ); } const application = await Application . findOne ({ domain }). exec (); if ( ! application ) { throw new ResourceNotFoundError ( `Application ${ domain } was not found.` ); } await redisClient . set ( key , JSON . stringify ( application . template . schema )); return application . template . schema ; }; Next, we create a service function that will insert the data payload inside the Mongo database. // src/services/dataService.ts export const createDataForApplication = async ( domain : string , validData : object ) : Promise < DataModel > => { const data = { domain , payload : validData } as DataModel ; await new Data ( data ). save (); return data ; };","title":"Service"},{"location":"development/server/#controller","text":"We can now create a controller function that will handle all the logic of the route handler. // src/controllers/dataController.ts export const create = async ( req : Request , res : Response , next : NextFunction ) => { const { domain } = req . params ; try { const schema = await getApplicationSchema ( domain ); validateDataWithTemplate ( req . body , schema ); const data = await createDataForApplication ( domain , req . body ); const response = new ResponseBuilder () . withStatusCode ( HttpStatus . CREATED ) . withData ( data ); res . status ( response . statusCode ). send ( response . build ()); } catch ( error ) { next ( error ); } }; All the logic is here. We fetch the schema for the current application, we validate that the request body conforms to the fetched schema, then we insert the data into the Mongo database and finally we respond to the requesting party with the data that was inserted.","title":"Controller"},{"location":"development/server/#route","text":"Finally, we register this controller in the appropriate router with the corresponding route and method. // src/routes/applicationRouter.ts router . route ( '/:domain/data' ) . get ( verifyUser , dataController . get ) . post ( jsonBodyRequired , dataController . create ) // This is the line that has been added in this case. . all ( onlySupportedMethods ( 'GET' , 'POST' )); And that's it. Since this route was defined inside the applicationRouter.ts file, the endpoint for this route will be: POST http://localhost:4000/api/applications/:domain/data","title":"Route"},{"location":"development/server/#creating-documentation","text":"The server has a documentation site which includes information of all the routes exposed in the API. Inside the server folder there is a documentation folder, which includes a script that generates the OpenAPI specification file that will contain all the data to render the documentation site. In order to create a new entry, you should: Inside the documentation/schemas folder, open the file corresponding to the router where you have created your route handler. Create a new RequestSchema (if your route requires a request body) and a ResponseSchema that contains the structure of the data returned by the route handler. These schemas are joi schemas and in this particular use case they only serve as a way to represent the requests and responses, and are not used for any sort of validation. Inside the documentation/routes folder, open the folder corresponding to the route of your created route handler and create a new file with the same name as the service that your route handler executes. The idea is that these files should be descriptive of what route exactly is it that they document. Inside this newly created file, export a RouteData object with all the information corresponding to your route. Inside the documentation/documentation.ts file, import your newly created route data file and include it inside the exported object in the paths object inside the corresponding object (depending on the name of the folder that you created your route data file in). These entries are in the same order that will be displayed in the documentation site, so keep that in mind.","title":"Creating Documentation"},{"location":"development/server/#routedata","text":"Your route data file should export an object of type RouteData which includes the following fields: path : The path of the route handler. ( Required string ) method : The method of the route handler. ( Required One of: get , put , post , delete , patch ) summary : A little summary of what the route handler does. ( Required string ) description : A more detailed description of what the route handler does. ( Required string ) throws : An array of the instances of the potential HttpError errors that your route handler may throw. ( Optional HttpError[] ) success : An object that describes the information of a successful response. ( Required ) success.code : The HTTP response code that your route handler responds with on successful response. ( Required number ) success.schema : The name of the ResponseSchema that your route handler responds with (if any). ( Optional string ) success.isArray : Whether the response object is an array of ResponseSchema objects or not. ( Optional boolean - Default: false ) success.binaryType : The MIME-type of the response in case the response is a binary blob. ( Optional string ) pathParams : An array of objects that describe each path parameter inside the route. ( Optional ) pathParams[].name : The name of the path parameter. ( Required string ) pathParams[].description : A description of what the path parameter represents. ( Required string ) pathParams[].type : The type of the path parameter. ( Required One of: string , number ) queryParams : An array of objects that describe each query parameter that can be used in the route. ( Optional ) queryParams[].name : The name of the query parameter. ( Required string ) queryParams[].description : A description of what the query parameter represents. ( Required string ) queryParams[].required : Whether the query parameter is required or not. ( Required boolean ) queryParams[].type : The type of the query parameter. ( Required One of: string , boolean , number ) queryParams[].isArray : Whether the query parameter is an array or not. ( Required boolean ) bodySchema : The name of the RequestSchema that your route handler needs as a JSON body (if any). ( Optional string ) tokenRequired : Whether the route handler requires a bearer token to be specified in the Authorization header. ( Optional boolean ) Note If your route has tokenRequired set to true, you need to add a UnauthorizedError instance inside throws .","title":"RouteData"},{"location":"development/server/#example-documentation","text":"Here we'll use the same example as used above, for the route that handles data upload.","title":"Example Documentation"},{"location":"development/server/#schema","text":"The relevant schemas are as following: // documentation/schemas/application.ts export const ApplicationDataRequestSchema = Joi . object ({ example : Joi.string (). required () }); export const ApplicationDataResponseSchema = Joi . object ({ domain : Joi.string (). required (), payload : ApplicationDataRequestSchema }); In this case, the example is really only an example. Since data can come in any shape or form, an example body was necessary.","title":"Schema"},{"location":"development/server/#routedata-file","text":"The file with the information of this route will then be: // documentation/routes/applications/createApplicationData.ts import HttpStatus from 'http-status-codes' ; import { RouteData } from '../../types' ; import { UnauthorizedRequestError , SchemaValidationError , ResourceNotFoundError } from '../../../src/errors/http' ; const data : RouteData = { path : '/applications/:domain/data' , method : 'post' , summary : 'Create a new data entry for an application for the requesting user.' , description : `Create a new data entry for an application for the requesting user. The shape of the request body will depend on the schema that was used to create the application.` , throws : [ new UnauthorizedRequestError (), new SchemaValidationError ( '' ), new ResourceNotFoundError ( '' ) ], success : { code : HttpStatus.CREATED , schema : 'ApplicationDataResponseSchema' }, pathParams : [ { name : 'domain' , description : 'The domain of the application to upload the data to.' , type : 'string' } ], bodySchema : 'ApplicationDataRequestSchema' , tokenRequired : true }; export default data ;","title":"RouteData File"},{"location":"development/server/#including-the-routedata-file","text":"Finally, we need to include this route data file into the documentation/documentation.ts file. Just import this file and add it into the exported object. // documentation/documentation.ts import createApplicationData from './routes/applications/createApplicationData' ; const documentationData = { metadata , info , paths : { Application : { getAllApplicationsForUser , createApplicationForUser , getApplicationByDomain , updateApplicationByDomain , deleteApplicationByDomain , createApplicationData , getApplicationData , getApplicationDataAsCsv } } }; export default documentationData ; The rest of the file was omitted as it is not relevant. Here you can see that createApplicationData is exported near the end of the paths.Application object.","title":"Including the RouteData File"},{"location":"development/web-dashboard/","text":"Web Dashboard Development \u00b6 Warning If you wish to develop the web dashboard, please head over to the Server Development page and follow the set up steps first. You need a local Freenalytics server when developing the web dashboard. The freenalytics/freenalytics repository is a monorepo containing both the web dashboard and the server. Any changes made to the web dashboard should be made in this repository. Requirements \u00b6 In order to develop for the web dashboard you will need: git Node.js (Version 16.15.1 was used) Setting Up \u00b6 First, clone the repository: git clone https://github.com/freenalytics/freenalytics Head over to the web-dashboard folder: cd web-dashboard And then, install the dependencies: npm install Setting Up the Development Environment \u00b6 As mentioned before, the web dashboard requires you to have a local server running on port 4000 . If you don't have the server set up properly yet, please visit the Server Development page. Once you have the server set up correctly, start it up with: npm run dev:watch Starting the Server \u00b6 To start the development server, run: npm run dev A React development server will start on port 3000 . This server has hot-reload capabilities, meaning that if you change a file while the server is running, the changes will be displayed live on your browser. Considerations \u00b6 Linting \u00b6 This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix Unit Testing \u00b6 This project contains unit tests for certain functionality. Visual components are currently not tested, only utilities are. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch Component Structure \u00b6 This project follows the following structure: Any style sheet should be imported in the src/index.tsx file. Any ContextProvider should be included in the src/App.tsx file. Page components should be placed inside src/pages/page_name/PageComponent.tsx . Components used only in a certain page should be placed inside src/components/pageComponents/page_name/component_name/Component.tsx . Components used in multiple places should be placed inside src/components/common/component_name/Component.tsx . Data visualization components should be placed inside src/components/dataVisualization/component_name/Component.tsx . Form components should be placed inside src/components/forms/form_name/FormComponent.tsx . Form control components should be placed inside src/components/common/form/component_name/Component.tsx . Component folder names should be in camelCase and component files should be in PascalCase and should have a .tsx extension. Inside each component folder there should be a index.ts file that proxy exports the relevant components inside the folder. Following this structure ensures that the code remains consistent. Inside any component, it is recommended to organize the imports in the following way: Anything from the react package. Any React component that comes from a third-party library. Any React hook that comes from a third-party library. Local React components. Local React hooks. Anything else. Component Code \u00b6 As mentioned before, component folder names should be in camelCase and component files should be in PascalCase and should have a .tsx extension. Inside each component folder there should be a index.ts file that proxy exports the relevant components inside the folder. Example Component \u00b6 Let's say we want to create a common component named MyComponent . We should then create a folder named src/components/common/myComponent and inside this folder we will create the following files: MyComponent.tsx : This file will contain the code relevant to the component itself. index.ts : This file will proxy export the component. (More on this later.) index.scss : This file will contain any styles relevant to this component. Note This folder may contain more components if they're closely related to MyComponent (for example, sub-components). Other files like typings can also be included in here. Note The index.scss file is not required if the component does not need any custom styles. myComponent/MyComponent.tsx \u00b6 Let's create the component itself first. import React , { useState } from 'react' ; interface Props { name : string } const MyComponent : React.FC < Props > = ({ name }) => { const [ count , setCount ] = useState < number > ( 0 ); const handleButtonClick = () => { setCount ( count + 1 ); }; return ( < div className = \"my-style\" > < p > Hello there { name } ! < /p> < button onClick = { handleButtonClick } > You have clicked this button { count } times . < /button> < /div> ); }; export default MyComponent ; This is a simple component that greets the user specified with the name prop. It also includes a button that when clicked will update the counter inside. This example illustrates how Props should be an interface that describes the props that MyComponent requires, and how event handlers should be defined inside the component with a name that begins with handle . myComponent/index.ts \u00b6 As mentioned before, this component proxy exports MyComponent.tsx . What this means is that this file should import MyComponent and then re-export it. Why? Because this way we can have components inside their own folder and keep them inside a file with a relevant name and not with index.tsx . This way, when we import this component we will use: import MyComponent from './components/common/myComponent' ; Instead of: import MyComponent from './components/common/myComponent/MyComponent' ; With that been said, the content of this index.ts file would be: import MyComponent from './MyComponent' ; export default MyComponent ; myComponent/index.scss \u00b6 Since our component has some styles associated to it, we need to create this file. An example of what these styles could look like is: .my-style { text-align : center ; p { color : red ; } button { font-weight : 700 ; color : gray ; } } In order to include these styles, it is necessary to add an import of this file inside the src/styles/main.scss file. @import '../components/common/myComponent Import order does matter and generally should be kept as: Common components Data visualization components Form components Page Components ( src/components/pageComponents ) Pages ( src/pages ) Page Code \u00b6 As with components, pages should also be located in their respective folders with the folder name being in camelCase and the page component file in PascalCase with a .tsx extension. Inside this folder there should also be a index.ts file that proxy exports the page component. Example Page \u00b6 For the sake of example, let's say we want to create a page named MyPage . We should then create a folder named src/pages/myPage and inside this folder we will create the following files: MyPage.tsx : This file will contain the code relevant to the page itself. index.ts : This file will proxy export the page component. index.scss : This file will contain any styles relevant to the page. Note The index.scss file is not required if the page component does not need any custom styles. myPage/MyPage.tsx \u00b6 Let's create the page component itself first. import React from 'react' ; import PageWrapper from '../../components/common/pageWrapper' ; import useTitle from '../../hooks/title' ; const MyPage : React.FC = () => { useTitle ( 'pages.my_page.title' ); const { t } = useLocale (); return ( < PageWrapper > < div className = \"my-page\" > This is my cool page . < /div> < /PageWrapper> ); }; export default MyPage ; This is a simple page that simply says \"This is my cool page.\". In this example, the component <PageWrapper> will include the navbar and the footer with your page. If your page should render the navbar and footer then wrap your content in this component, if not you can omit it. Note Notice the string passed to useTitle() . This hook will update the title of the page with a string located in the src/i18n/strings resource folder. We still haven't seen how this project tackles Localization so keep reading, it will then be explained. Additionally, an entry inside src/i18n/strings/en.ts will be inserted, inside the PAGES object, with the key: 'pages.my_page.title' And with a value that should include the title of the page, which will be then displayed in the browser's window and tab. myPage/index.ts \u00b6 As it is the case for regular components, page components need to be proxy exported too. This file should then look like this: import MyPage from './MyPage' ; export default MyPage ; myPage/index.scss \u00b6 Since our page component has some styles associated to it, we need to create this file. An example of what these styles could look like is: .my-page { text-align : center ; color : white ; padding : 2rem ; } In order to include these styles, it is necessary to add an import of this file inside the src/styles/main.scss file. @import '../pages/myPage This import should be added at the end of the file because we want to import page styles at the end. Adding to the Router \u00b6 Once the page component has been created, it is time to add it to the router. Currently, all routes are located in the src/constants/routes.ts file and are separated by access type. If the route is accessible by any user (whether they're logged in or not) then the route should be inside the PUBLIC_ROUTES object. Otherwise, if the user should be logged in to see the page, then add the route inside PROTECTED_ROUTES . Additionally, if your route is dynamic, meaning that it has a parameter in it, then create a function inside the DYNAMIC_PROTECTED_ROUTES that returns the route with the parameter applied to it. Once this is done, we can now import the page component inside our router and add a <Route> that renders this page component. For this, enter the src/router/Router.tsx file and modify it to import your page component: import MyPage from '../pages/myPage' ; And add the route to it. If the page is public, then add the route like this: < Route path = { PUBLIC_ROUTES . myPage } element = { < MyPage /> } /> If it's protected, then add the route like this: < Route element = { < ProtectedRoute allowed = { loggedIn } redirectPath = { PUBLIC_ROUTES . login } /> } > { /* ... */ } < Route path = { PROTECTED_ROUTES . myPage } element = { < MyPage /> } /> < /Route> If the route is dynamic and protected, then add the route like this: < Route element = { < ProtectedRoute allowed = { loggedIn } redirectPath = { PUBLIC_ROUTES . login } /> } > { /* ... */ } < Route path = { DYNAMIC_PROTECTED_ROUTES . myPage ( ':param' )} element = { < MyPage /> } /> < /Route> Localization \u00b6 You may have come across the weird string passed to the useTitle() hook in myPage/MyPage.tsx . This string is nothing more than a key that maps to an actual string that will be displayed on the page. Why? Localization. In the future, this project will be available in more languages other than English. For the time being, these strings will only exist in English but when more languages will be supported, each of these strings will need to be translated and added into their own files. Adding Strings \u00b6 To add strings to the localizations service, open the src/i18n/strings folder and access the file that you wish to edit. In this case, only English strings are available, so the file that we will update is en.ts . In here, you will find multiple objects. Each object serves as a way to organize these strings by type. You can find an object for: Strings inside pages. ( PAGES ) Strings inside common components. ( COMMON ) Strings inside forms. ( FORMS ) Strings inside error messages. ( ERRORS ) Each key should be well structured so it can clearly reflect where it is being used. Generally, the key should have this structure: <type>.<page>.<component>.<name>.<type_of_string> So, in this case, a component used in a page named myPage that is displayed inside a component named myComponent that is displayed in a button that should say \"Accept\" and that is part of a group of buttons should be: pages.my_page.my_component.buttons.accept.text The naming can be a bit subjective and maybe confusing, but as long as it extensively describes its usage it should be fine. The values of these strings should be in ICU Format . This allows for dynamic variables to be inserted, including modifying the text in cases where plural and singular versions differ. For example, if you want to include a variable named name inside a text, you can do so by defining it as: \"Hello, my name is {name}.\" Using Strings \u00b6 Using these strings inside the components is as easy as importing the useLocale() hook and calling the t() function with the key of the string to use. import useLocale from '../../hooks/locale' ; const { t } = useLocale (); t ( 'common.my_string.text' ) In the case that common.my_string.text includes a variable named name inside it, you can include an object in the t() call to include all the named variables in the resulting text. t ( 'common.my_string.text' , { name : 'my name' }) Using the API \u00b6 To use the API, a service file should be created inside the src/services/api folder, or use an already existing one. Services are separated by the type of entities that are used. In this case, applications and authenticate are the only entities available, so there is only ApplicationService.ts and AuthService.ts . These service files typically include interfaces to type the responses of the API, along with methods that call the API through Axios. Methods are separated in two, one method that makes the API call and the other one that returns an object with a key associated to the data fetched or mutated, and the function reference of the method that makes the API call. Why? This project uses TanStack Query to cache the responses. Example Request \u00b6 Let's say we want to create a GET request to the /applications/:id endpoint. In this case, since we're talking about an application we should check the ApplicationService.ts file. Inside, we should create two methods as such: private async doGetApplicationByDomain ( domain : string ) : Promise < ApplicationModel > { try { const response = await this . client . instance . get ( `/applications/ ${ domain } ` ); return response . data . data ; } catch ( error ) { this . client . handleRequestError ( error ); throw this . client . createRequestError ( error ); } } public getApplicationByDomain ( domain : string ) { return { key : [ 'applications' , domain ], fn : () => this . doGetApplicationByDomain ( domain ) }; } In this case, doGetApplicationByDomain() fetches the application data through the Axios instance in this.client.instance and returns the response. The getApplicationByDomain() returns an object with the key that will be used to cache this response and with a function reference that will actually fetch the data. This will be visited again in making requests inside components . Notice how the doGetApplicationByDomain() method returns a Promise<ApplicationModel> . This is an interface defined in this same file and could be defined as such (this is what the API returns): export const VALID_APPLICATION_TYPES = [ 'mobile' , 'web' , 'server' , 'desktop' , 'other' ] as const ; export type ApplicationType = typeof VALID_APPLICATION_TYPES [ number ]; export interface TemplateModel { raw_schema : string schema : object } export interface ConnectorModel { package_url : string language : string } export interface ApplicationModel { name : string owner : string type : ApplicationType domain : string template : TemplateModel connectors : ConnectorModel [] createdAt : string lastModifiedAt : string } It is recommended to type the responses to avoid potential bugs when consuming these services. Making Requests Inside Components \u00b6 Components that make requests should only do requests and present the data obtained through another component who's responsibility is to display this data. An example of what type of components could make fetch requests would be pages since technically they shouldn't render any visuals which should be rendered by other components instead. Anyways, let's use the above example to make a request that will get the data for a particular application. The page component that displays the application data should then import the following hooks and components: import { useQuery } from '@tanstack/react-query' ; import Loading from '../../components/common/loading' ; import RequestErrorMessageFullPage from '../../components/common/requestErrorMessageFullPage' ; import useApi from '../../hooks/api' ; And can then make the requests inside like: const ApplicationPage : React.FC = () => { const { client } = useApi (); const request = client . application . getApplicationByDomain ( domain ); // Domain could be acquired through the router params with useParams(). const { isLoading , error , data : application } = useQuery ( request . key , request . fn ); if ( isLoading ) { return ( < Loading /> ); } if ( error ) { return ( < RequestErrorMessageFullPage error = { error as Error } /> ); } return ( < PageWrapper > < MyApplication application = { application } /> < /PageWrapper> ); }; Notice how in this case, this component does not render anything visual with the data, it instead renders <MyApplication> which in itself is the one responsible of rendering the application data. As for the error, if the error should be displayed in full screen then the <RequestErrorMessageFullPage> component should be used. Otherwise, use <RequestErrorMessage> . Creating Forms \u00b6 Forms are an essential part of any web application and they can be quite cumbersome to implement them in a clean and modular way. In this project, the following structure is used for any form. A form component named MyForm.tsx will do any of the requests necessary to fetch data or to upload the form data. This component will render the next component. A form logic component named MyFormLogic.tsx will do any sort of validation prior to sending data, it will define the structure of the data that will be uploaded. This component will render the next component. A form view component named MyFormView.tsx will do the actual rendering of the form. An index.ts file that will proxy export MyForm.tsx . A types.ts file that will contain the interfaces of the request bodies (if used by a service) or any other structure that needs typing. This ensures that the components follow a proper separation of concerns. Any form should be placed in its own folder inside src/components/forms . Example Form \u00b6 Let's check an example. Let's say we want to create a form named MyForm . For this, let's create a folder inside src/components/forms named myForm . myForm/types.ts \u00b6 Let's define first the shape of the data that will be uploaded inside the types.ts file: export interface MyData { name : string age : number } myForm/MyForm.tsx \u00b6 Let's define the component that will do all the requests. import React from 'react' ; import { useMutation } from '@tanstack/react-query' ; import Loading from '../../common/loading' ; import RequestErrorMessageFullPage from '../../common/requestErrorMessageFullPage' ; import MyFormLogic from './MyFormLogic' ; import useApi from '../../../hooks/api' ; import { MyData } from './types' ; const MyForm : React.FC = () => { const { client } = useApi (); const request = client . application . postMyData (); const mutation = useMutation ( request . key , request . fn ); const handleSubmit = async ( data : MyData ) => { await mutation . mutateAsync ( data ); }; return ( < MyFormLogic onSubmit = { handleSubmit } /> ); }; export default MyForm ; This assumes a postMyData() method exists in client.application as a service. This is obviously not the case and is only here as an illustration. myForm/MyFormLogic.tsx \u00b6 Now, it's time to implement the logic component that will validate that the data inserted is correct. import React , { useState } from 'react' ; import { useForm , SubmitHandler } from 'react-hook-form' ; import { joiResolver } from '@hookform/resolvers/joi' ; import Joi from 'joi' ; import MyFormView from './MyFormView' ; import useLocale from '../../../hooks/locale' ; import { MyData } from './types' ; interface Props { onSubmit : SubmitHandler < MyData > } const MyFormLogic : React.FC < Props > = ({ onSubmit }) => { const { t } = useLocale (); const CreateMyDataSchema = Joi . object < MyData > ({ name : Joi.string (). min ( 1 ). trim (). required () . messages ({ 'string.min' : t ( 'forms.my_form.errors.fields.name.min' ), 'any.required' : t ( 'forms.my_form.errors.fields.name.required' ), 'string.empty' : t ( 'forms.my_form.errors.fields.name.required' ) }), age : Joi.number (). integer (). positive (). required () . messages ({ 'number.base' : t ( 'forms.my_form.errors.fields.age.base' ), 'number.positive' : t ( 'forms.my_form.errors.fields.age.positive' ), 'any.required' : t ( 'forms.my_form.errors.fields.age.required' ) }) }); const form = useForm < MyData > ({ mode : 'onSubmit' , resolver : joiResolver ( CreateMyDataSchema ) }); const [ error , setError ] = useState < Error | null > ( null ); const handleSubmit = async ( data : MyData ) => { try { await onSubmit ( data ); } catch ( error ) { setError ( error as Error ); } }; return ( < MyFormView form = { form } onSubmit = { handleSubmit } error = { error } /> ); }; export default MyFormLogic ; Notice how the CreateMyDataSchema contains localized string keys in the messages() method. This messages() method will define the validation error messages of the Joi schema used. In this case, inside the src/i18n/strings/en.ts file we should add the following strings inside the FORMS object: 'forms.my_form.errors.fields.name.min' : 'Your name should be at least 1 character long.' , 'forms.my_form.errors.fields.name.required' : 'Your name is required.' , 'forms.my_form.errors.fields.age.base' : 'Your age should be an integer.' , 'forms.my_form.errors.fields.age.positive' : 'Your age should be a positive integer.' , 'forms.my_form.errors.fields.age.required' : 'Your age is required.' , Needless to say, notice the structure of these keys. We already covered on why it was important to keep a concise naming convention for these keys. In this case, these keys convey the following information: They are used in a form because of the forms . They are used in MyForm because of the my_form . They are used in error messages because of the errors . They are used for errors related to a fields in the form because of fields . They are used in the name and age fields because of the name and age . They represent a specific error because of the min , required , base and positive . myForm/MyFormView.tsx \u00b6 Lastly, we need to create the form view. import React from 'react' ; import { Box , Form , Button } from 'react-bulma-components' ; import { SubmitHandler , UseFormReturn } from 'react-hook-form' ; import RequestErrorMessage from '../../common/requestErrorMessage' ; import useLocale from '../../../hooks/locale' ; import useFormHelper from '../../../hooks/formHelper' ; import { MyData } from './types' ; interface Props { form : UseFormReturn < MyData > onSubmit : SubmitHandler < MyData > error? : Error | null } const MyFormView : React.FC < Props > = ({ form , onSubmit , error }) => { const { t } = useLocale (); const { handleChangeNoValidation , handleBlurValidate } = useFormHelper < MyData > ( form ); const { handleSubmit , formState : { isSubmitting , errors } } = form ; return ( < Box > < form onSubmit = { handleSubmit ( onSubmit )} > < RequestErrorMessage error = { error } /> < Form . Field > < Form . Label > { t ( 'forms.my_form.name.label' )} < /Form.Label> < Form . Control > < Form . Input type = \"text\" name = \"name\" onChange = { handleChangeNoValidation } onBlur = { handleBlurValidate } /> < /Form.Control> < Form . Help color = \"danger\" > { errors . name ? . message } < /Form.Help> < /Form.Field> < Form . Field > < Form . Label > { t ( 'forms.my_form.age.label' )} < /Form.Label> < Form . Control > < Form . Input type = \"number\" name = \"age\" onChange = { handleChangeNoValidation } onBlur = { handleBlurValidate } /> < /Form.Control> < Form . Help color = \"danger\" > { errors . age ? . message } < /Form.Help> < /Form.Field> < Button . Group align = \"right\" > < Button color = \"primary\" submit loading = { isSubmitting } > { t ( 'forms.my_form.buttons.submit.label' )} < /Button> < /Button.Group> < /form> < /Box> ); }; export default MyFormView ; In this case, what ties the form control with the data it controls is the name prop. Notice that the name prop is the same as the one we defined in the CreateMyDataSchema . handleChangeNoValidation() and handleBlurValidate() are just functions exported by the useFormHelper() hook that allow to validate the data once the user loses focus on the input. This file also takes into account that the following strings need to be added in src/i18n/strings/en.ts inside the FORMS object: 'forms.my_form.name.label' : 'Your Name' , 'forms.my_form.age.label' : 'Your Age' , 'forms.my_form.buttons.submit.label' : 'Submit' , myForm/index.ts \u00b6 Finally, we can proxy export the MyForm.tsx component: import MyForm from './MyForm' ; export default MyForm ;","title":"Web Dashboard"},{"location":"development/web-dashboard/#web-dashboard-development","text":"Warning If you wish to develop the web dashboard, please head over to the Server Development page and follow the set up steps first. You need a local Freenalytics server when developing the web dashboard. The freenalytics/freenalytics repository is a monorepo containing both the web dashboard and the server. Any changes made to the web dashboard should be made in this repository.","title":"Web Dashboard Development"},{"location":"development/web-dashboard/#requirements","text":"In order to develop for the web dashboard you will need: git Node.js (Version 16.15.1 was used)","title":"Requirements"},{"location":"development/web-dashboard/#setting-up","text":"First, clone the repository: git clone https://github.com/freenalytics/freenalytics Head over to the web-dashboard folder: cd web-dashboard And then, install the dependencies: npm install","title":"Setting Up"},{"location":"development/web-dashboard/#setting-up-the-development-environment","text":"As mentioned before, the web dashboard requires you to have a local server running on port 4000 . If you don't have the server set up properly yet, please visit the Server Development page. Once you have the server set up correctly, start it up with: npm run dev:watch","title":"Setting Up the Development Environment"},{"location":"development/web-dashboard/#starting-the-server","text":"To start the development server, run: npm run dev A React development server will start on port 3000 . This server has hot-reload capabilities, meaning that if you change a file while the server is running, the changes will be displayed live on your browser.","title":"Starting the Server"},{"location":"development/web-dashboard/#considerations","text":"","title":"Considerations"},{"location":"development/web-dashboard/#linting","text":"This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix","title":"Linting"},{"location":"development/web-dashboard/#unit-testing","text":"This project contains unit tests for certain functionality. Visual components are currently not tested, only utilities are. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch","title":"Unit Testing"},{"location":"development/web-dashboard/#component-structure","text":"This project follows the following structure: Any style sheet should be imported in the src/index.tsx file. Any ContextProvider should be included in the src/App.tsx file. Page components should be placed inside src/pages/page_name/PageComponent.tsx . Components used only in a certain page should be placed inside src/components/pageComponents/page_name/component_name/Component.tsx . Components used in multiple places should be placed inside src/components/common/component_name/Component.tsx . Data visualization components should be placed inside src/components/dataVisualization/component_name/Component.tsx . Form components should be placed inside src/components/forms/form_name/FormComponent.tsx . Form control components should be placed inside src/components/common/form/component_name/Component.tsx . Component folder names should be in camelCase and component files should be in PascalCase and should have a .tsx extension. Inside each component folder there should be a index.ts file that proxy exports the relevant components inside the folder. Following this structure ensures that the code remains consistent. Inside any component, it is recommended to organize the imports in the following way: Anything from the react package. Any React component that comes from a third-party library. Any React hook that comes from a third-party library. Local React components. Local React hooks. Anything else.","title":"Component Structure"},{"location":"development/web-dashboard/#component-code","text":"As mentioned before, component folder names should be in camelCase and component files should be in PascalCase and should have a .tsx extension. Inside each component folder there should be a index.ts file that proxy exports the relevant components inside the folder.","title":"Component Code"},{"location":"development/web-dashboard/#example-component","text":"Let's say we want to create a common component named MyComponent . We should then create a folder named src/components/common/myComponent and inside this folder we will create the following files: MyComponent.tsx : This file will contain the code relevant to the component itself. index.ts : This file will proxy export the component. (More on this later.) index.scss : This file will contain any styles relevant to this component. Note This folder may contain more components if they're closely related to MyComponent (for example, sub-components). Other files like typings can also be included in here. Note The index.scss file is not required if the component does not need any custom styles.","title":"Example Component"},{"location":"development/web-dashboard/#mycomponentmycomponenttsx","text":"Let's create the component itself first. import React , { useState } from 'react' ; interface Props { name : string } const MyComponent : React.FC < Props > = ({ name }) => { const [ count , setCount ] = useState < number > ( 0 ); const handleButtonClick = () => { setCount ( count + 1 ); }; return ( < div className = \"my-style\" > < p > Hello there { name } ! < /p> < button onClick = { handleButtonClick } > You have clicked this button { count } times . < /button> < /div> ); }; export default MyComponent ; This is a simple component that greets the user specified with the name prop. It also includes a button that when clicked will update the counter inside. This example illustrates how Props should be an interface that describes the props that MyComponent requires, and how event handlers should be defined inside the component with a name that begins with handle .","title":"myComponent/MyComponent.tsx"},{"location":"development/web-dashboard/#mycomponentindexts","text":"As mentioned before, this component proxy exports MyComponent.tsx . What this means is that this file should import MyComponent and then re-export it. Why? Because this way we can have components inside their own folder and keep them inside a file with a relevant name and not with index.tsx . This way, when we import this component we will use: import MyComponent from './components/common/myComponent' ; Instead of: import MyComponent from './components/common/myComponent/MyComponent' ; With that been said, the content of this index.ts file would be: import MyComponent from './MyComponent' ; export default MyComponent ;","title":"myComponent/index.ts"},{"location":"development/web-dashboard/#mycomponentindexscss","text":"Since our component has some styles associated to it, we need to create this file. An example of what these styles could look like is: .my-style { text-align : center ; p { color : red ; } button { font-weight : 700 ; color : gray ; } } In order to include these styles, it is necessary to add an import of this file inside the src/styles/main.scss file. @import '../components/common/myComponent Import order does matter and generally should be kept as: Common components Data visualization components Form components Page Components ( src/components/pageComponents ) Pages ( src/pages )","title":"myComponent/index.scss"},{"location":"development/web-dashboard/#page-code","text":"As with components, pages should also be located in their respective folders with the folder name being in camelCase and the page component file in PascalCase with a .tsx extension. Inside this folder there should also be a index.ts file that proxy exports the page component.","title":"Page Code"},{"location":"development/web-dashboard/#example-page","text":"For the sake of example, let's say we want to create a page named MyPage . We should then create a folder named src/pages/myPage and inside this folder we will create the following files: MyPage.tsx : This file will contain the code relevant to the page itself. index.ts : This file will proxy export the page component. index.scss : This file will contain any styles relevant to the page. Note The index.scss file is not required if the page component does not need any custom styles.","title":"Example Page"},{"location":"development/web-dashboard/#mypagemypagetsx","text":"Let's create the page component itself first. import React from 'react' ; import PageWrapper from '../../components/common/pageWrapper' ; import useTitle from '../../hooks/title' ; const MyPage : React.FC = () => { useTitle ( 'pages.my_page.title' ); const { t } = useLocale (); return ( < PageWrapper > < div className = \"my-page\" > This is my cool page . < /div> < /PageWrapper> ); }; export default MyPage ; This is a simple page that simply says \"This is my cool page.\". In this example, the component <PageWrapper> will include the navbar and the footer with your page. If your page should render the navbar and footer then wrap your content in this component, if not you can omit it. Note Notice the string passed to useTitle() . This hook will update the title of the page with a string located in the src/i18n/strings resource folder. We still haven't seen how this project tackles Localization so keep reading, it will then be explained. Additionally, an entry inside src/i18n/strings/en.ts will be inserted, inside the PAGES object, with the key: 'pages.my_page.title' And with a value that should include the title of the page, which will be then displayed in the browser's window and tab.","title":"myPage/MyPage.tsx"},{"location":"development/web-dashboard/#mypageindexts","text":"As it is the case for regular components, page components need to be proxy exported too. This file should then look like this: import MyPage from './MyPage' ; export default MyPage ;","title":"myPage/index.ts"},{"location":"development/web-dashboard/#mypageindexscss","text":"Since our page component has some styles associated to it, we need to create this file. An example of what these styles could look like is: .my-page { text-align : center ; color : white ; padding : 2rem ; } In order to include these styles, it is necessary to add an import of this file inside the src/styles/main.scss file. @import '../pages/myPage This import should be added at the end of the file because we want to import page styles at the end.","title":"myPage/index.scss"},{"location":"development/web-dashboard/#adding-to-the-router","text":"Once the page component has been created, it is time to add it to the router. Currently, all routes are located in the src/constants/routes.ts file and are separated by access type. If the route is accessible by any user (whether they're logged in or not) then the route should be inside the PUBLIC_ROUTES object. Otherwise, if the user should be logged in to see the page, then add the route inside PROTECTED_ROUTES . Additionally, if your route is dynamic, meaning that it has a parameter in it, then create a function inside the DYNAMIC_PROTECTED_ROUTES that returns the route with the parameter applied to it. Once this is done, we can now import the page component inside our router and add a <Route> that renders this page component. For this, enter the src/router/Router.tsx file and modify it to import your page component: import MyPage from '../pages/myPage' ; And add the route to it. If the page is public, then add the route like this: < Route path = { PUBLIC_ROUTES . myPage } element = { < MyPage /> } /> If it's protected, then add the route like this: < Route element = { < ProtectedRoute allowed = { loggedIn } redirectPath = { PUBLIC_ROUTES . login } /> } > { /* ... */ } < Route path = { PROTECTED_ROUTES . myPage } element = { < MyPage /> } /> < /Route> If the route is dynamic and protected, then add the route like this: < Route element = { < ProtectedRoute allowed = { loggedIn } redirectPath = { PUBLIC_ROUTES . login } /> } > { /* ... */ } < Route path = { DYNAMIC_PROTECTED_ROUTES . myPage ( ':param' )} element = { < MyPage /> } /> < /Route>","title":"Adding to the Router"},{"location":"development/web-dashboard/#localization","text":"You may have come across the weird string passed to the useTitle() hook in myPage/MyPage.tsx . This string is nothing more than a key that maps to an actual string that will be displayed on the page. Why? Localization. In the future, this project will be available in more languages other than English. For the time being, these strings will only exist in English but when more languages will be supported, each of these strings will need to be translated and added into their own files.","title":"Localization"},{"location":"development/web-dashboard/#adding-strings","text":"To add strings to the localizations service, open the src/i18n/strings folder and access the file that you wish to edit. In this case, only English strings are available, so the file that we will update is en.ts . In here, you will find multiple objects. Each object serves as a way to organize these strings by type. You can find an object for: Strings inside pages. ( PAGES ) Strings inside common components. ( COMMON ) Strings inside forms. ( FORMS ) Strings inside error messages. ( ERRORS ) Each key should be well structured so it can clearly reflect where it is being used. Generally, the key should have this structure: <type>.<page>.<component>.<name>.<type_of_string> So, in this case, a component used in a page named myPage that is displayed inside a component named myComponent that is displayed in a button that should say \"Accept\" and that is part of a group of buttons should be: pages.my_page.my_component.buttons.accept.text The naming can be a bit subjective and maybe confusing, but as long as it extensively describes its usage it should be fine. The values of these strings should be in ICU Format . This allows for dynamic variables to be inserted, including modifying the text in cases where plural and singular versions differ. For example, if you want to include a variable named name inside a text, you can do so by defining it as: \"Hello, my name is {name}.\"","title":"Adding Strings"},{"location":"development/web-dashboard/#using-strings","text":"Using these strings inside the components is as easy as importing the useLocale() hook and calling the t() function with the key of the string to use. import useLocale from '../../hooks/locale' ; const { t } = useLocale (); t ( 'common.my_string.text' ) In the case that common.my_string.text includes a variable named name inside it, you can include an object in the t() call to include all the named variables in the resulting text. t ( 'common.my_string.text' , { name : 'my name' })","title":"Using Strings"},{"location":"development/web-dashboard/#using-the-api","text":"To use the API, a service file should be created inside the src/services/api folder, or use an already existing one. Services are separated by the type of entities that are used. In this case, applications and authenticate are the only entities available, so there is only ApplicationService.ts and AuthService.ts . These service files typically include interfaces to type the responses of the API, along with methods that call the API through Axios. Methods are separated in two, one method that makes the API call and the other one that returns an object with a key associated to the data fetched or mutated, and the function reference of the method that makes the API call. Why? This project uses TanStack Query to cache the responses.","title":"Using the API"},{"location":"development/web-dashboard/#example-request","text":"Let's say we want to create a GET request to the /applications/:id endpoint. In this case, since we're talking about an application we should check the ApplicationService.ts file. Inside, we should create two methods as such: private async doGetApplicationByDomain ( domain : string ) : Promise < ApplicationModel > { try { const response = await this . client . instance . get ( `/applications/ ${ domain } ` ); return response . data . data ; } catch ( error ) { this . client . handleRequestError ( error ); throw this . client . createRequestError ( error ); } } public getApplicationByDomain ( domain : string ) { return { key : [ 'applications' , domain ], fn : () => this . doGetApplicationByDomain ( domain ) }; } In this case, doGetApplicationByDomain() fetches the application data through the Axios instance in this.client.instance and returns the response. The getApplicationByDomain() returns an object with the key that will be used to cache this response and with a function reference that will actually fetch the data. This will be visited again in making requests inside components . Notice how the doGetApplicationByDomain() method returns a Promise<ApplicationModel> . This is an interface defined in this same file and could be defined as such (this is what the API returns): export const VALID_APPLICATION_TYPES = [ 'mobile' , 'web' , 'server' , 'desktop' , 'other' ] as const ; export type ApplicationType = typeof VALID_APPLICATION_TYPES [ number ]; export interface TemplateModel { raw_schema : string schema : object } export interface ConnectorModel { package_url : string language : string } export interface ApplicationModel { name : string owner : string type : ApplicationType domain : string template : TemplateModel connectors : ConnectorModel [] createdAt : string lastModifiedAt : string } It is recommended to type the responses to avoid potential bugs when consuming these services.","title":"Example Request"},{"location":"development/web-dashboard/#making-requests-inside-components","text":"Components that make requests should only do requests and present the data obtained through another component who's responsibility is to display this data. An example of what type of components could make fetch requests would be pages since technically they shouldn't render any visuals which should be rendered by other components instead. Anyways, let's use the above example to make a request that will get the data for a particular application. The page component that displays the application data should then import the following hooks and components: import { useQuery } from '@tanstack/react-query' ; import Loading from '../../components/common/loading' ; import RequestErrorMessageFullPage from '../../components/common/requestErrorMessageFullPage' ; import useApi from '../../hooks/api' ; And can then make the requests inside like: const ApplicationPage : React.FC = () => { const { client } = useApi (); const request = client . application . getApplicationByDomain ( domain ); // Domain could be acquired through the router params with useParams(). const { isLoading , error , data : application } = useQuery ( request . key , request . fn ); if ( isLoading ) { return ( < Loading /> ); } if ( error ) { return ( < RequestErrorMessageFullPage error = { error as Error } /> ); } return ( < PageWrapper > < MyApplication application = { application } /> < /PageWrapper> ); }; Notice how in this case, this component does not render anything visual with the data, it instead renders <MyApplication> which in itself is the one responsible of rendering the application data. As for the error, if the error should be displayed in full screen then the <RequestErrorMessageFullPage> component should be used. Otherwise, use <RequestErrorMessage> .","title":"Making Requests Inside Components"},{"location":"development/web-dashboard/#creating-forms","text":"Forms are an essential part of any web application and they can be quite cumbersome to implement them in a clean and modular way. In this project, the following structure is used for any form. A form component named MyForm.tsx will do any of the requests necessary to fetch data or to upload the form data. This component will render the next component. A form logic component named MyFormLogic.tsx will do any sort of validation prior to sending data, it will define the structure of the data that will be uploaded. This component will render the next component. A form view component named MyFormView.tsx will do the actual rendering of the form. An index.ts file that will proxy export MyForm.tsx . A types.ts file that will contain the interfaces of the request bodies (if used by a service) or any other structure that needs typing. This ensures that the components follow a proper separation of concerns. Any form should be placed in its own folder inside src/components/forms .","title":"Creating Forms"},{"location":"development/web-dashboard/#example-form","text":"Let's check an example. Let's say we want to create a form named MyForm . For this, let's create a folder inside src/components/forms named myForm .","title":"Example Form"},{"location":"development/web-dashboard/#myformtypests","text":"Let's define first the shape of the data that will be uploaded inside the types.ts file: export interface MyData { name : string age : number }","title":"myForm/types.ts"},{"location":"development/web-dashboard/#myformmyformtsx","text":"Let's define the component that will do all the requests. import React from 'react' ; import { useMutation } from '@tanstack/react-query' ; import Loading from '../../common/loading' ; import RequestErrorMessageFullPage from '../../common/requestErrorMessageFullPage' ; import MyFormLogic from './MyFormLogic' ; import useApi from '../../../hooks/api' ; import { MyData } from './types' ; const MyForm : React.FC = () => { const { client } = useApi (); const request = client . application . postMyData (); const mutation = useMutation ( request . key , request . fn ); const handleSubmit = async ( data : MyData ) => { await mutation . mutateAsync ( data ); }; return ( < MyFormLogic onSubmit = { handleSubmit } /> ); }; export default MyForm ; This assumes a postMyData() method exists in client.application as a service. This is obviously not the case and is only here as an illustration.","title":"myForm/MyForm.tsx"},{"location":"development/web-dashboard/#myformmyformlogictsx","text":"Now, it's time to implement the logic component that will validate that the data inserted is correct. import React , { useState } from 'react' ; import { useForm , SubmitHandler } from 'react-hook-form' ; import { joiResolver } from '@hookform/resolvers/joi' ; import Joi from 'joi' ; import MyFormView from './MyFormView' ; import useLocale from '../../../hooks/locale' ; import { MyData } from './types' ; interface Props { onSubmit : SubmitHandler < MyData > } const MyFormLogic : React.FC < Props > = ({ onSubmit }) => { const { t } = useLocale (); const CreateMyDataSchema = Joi . object < MyData > ({ name : Joi.string (). min ( 1 ). trim (). required () . messages ({ 'string.min' : t ( 'forms.my_form.errors.fields.name.min' ), 'any.required' : t ( 'forms.my_form.errors.fields.name.required' ), 'string.empty' : t ( 'forms.my_form.errors.fields.name.required' ) }), age : Joi.number (). integer (). positive (). required () . messages ({ 'number.base' : t ( 'forms.my_form.errors.fields.age.base' ), 'number.positive' : t ( 'forms.my_form.errors.fields.age.positive' ), 'any.required' : t ( 'forms.my_form.errors.fields.age.required' ) }) }); const form = useForm < MyData > ({ mode : 'onSubmit' , resolver : joiResolver ( CreateMyDataSchema ) }); const [ error , setError ] = useState < Error | null > ( null ); const handleSubmit = async ( data : MyData ) => { try { await onSubmit ( data ); } catch ( error ) { setError ( error as Error ); } }; return ( < MyFormView form = { form } onSubmit = { handleSubmit } error = { error } /> ); }; export default MyFormLogic ; Notice how the CreateMyDataSchema contains localized string keys in the messages() method. This messages() method will define the validation error messages of the Joi schema used. In this case, inside the src/i18n/strings/en.ts file we should add the following strings inside the FORMS object: 'forms.my_form.errors.fields.name.min' : 'Your name should be at least 1 character long.' , 'forms.my_form.errors.fields.name.required' : 'Your name is required.' , 'forms.my_form.errors.fields.age.base' : 'Your age should be an integer.' , 'forms.my_form.errors.fields.age.positive' : 'Your age should be a positive integer.' , 'forms.my_form.errors.fields.age.required' : 'Your age is required.' , Needless to say, notice the structure of these keys. We already covered on why it was important to keep a concise naming convention for these keys. In this case, these keys convey the following information: They are used in a form because of the forms . They are used in MyForm because of the my_form . They are used in error messages because of the errors . They are used for errors related to a fields in the form because of fields . They are used in the name and age fields because of the name and age . They represent a specific error because of the min , required , base and positive .","title":"myForm/MyFormLogic.tsx"},{"location":"development/web-dashboard/#myformmyformviewtsx","text":"Lastly, we need to create the form view. import React from 'react' ; import { Box , Form , Button } from 'react-bulma-components' ; import { SubmitHandler , UseFormReturn } from 'react-hook-form' ; import RequestErrorMessage from '../../common/requestErrorMessage' ; import useLocale from '../../../hooks/locale' ; import useFormHelper from '../../../hooks/formHelper' ; import { MyData } from './types' ; interface Props { form : UseFormReturn < MyData > onSubmit : SubmitHandler < MyData > error? : Error | null } const MyFormView : React.FC < Props > = ({ form , onSubmit , error }) => { const { t } = useLocale (); const { handleChangeNoValidation , handleBlurValidate } = useFormHelper < MyData > ( form ); const { handleSubmit , formState : { isSubmitting , errors } } = form ; return ( < Box > < form onSubmit = { handleSubmit ( onSubmit )} > < RequestErrorMessage error = { error } /> < Form . Field > < Form . Label > { t ( 'forms.my_form.name.label' )} < /Form.Label> < Form . Control > < Form . Input type = \"text\" name = \"name\" onChange = { handleChangeNoValidation } onBlur = { handleBlurValidate } /> < /Form.Control> < Form . Help color = \"danger\" > { errors . name ? . message } < /Form.Help> < /Form.Field> < Form . Field > < Form . Label > { t ( 'forms.my_form.age.label' )} < /Form.Label> < Form . Control > < Form . Input type = \"number\" name = \"age\" onChange = { handleChangeNoValidation } onBlur = { handleBlurValidate } /> < /Form.Control> < Form . Help color = \"danger\" > { errors . age ? . message } < /Form.Help> < /Form.Field> < Button . Group align = \"right\" > < Button color = \"primary\" submit loading = { isSubmitting } > { t ( 'forms.my_form.buttons.submit.label' )} < /Button> < /Button.Group> < /form> < /Box> ); }; export default MyFormView ; In this case, what ties the form control with the data it controls is the name prop. Notice that the name prop is the same as the one we defined in the CreateMyDataSchema . handleChangeNoValidation() and handleBlurValidate() are just functions exported by the useFormHelper() hook that allow to validate the data once the user loses focus on the input. This file also takes into account that the following strings need to be added in src/i18n/strings/en.ts inside the FORMS object: 'forms.my_form.name.label' : 'Your Name' , 'forms.my_form.age.label' : 'Your Age' , 'forms.my_form.buttons.submit.label' : 'Submit' ,","title":"myForm/MyFormView.tsx"},{"location":"development/web-dashboard/#myformindexts","text":"Finally, we can proxy export the MyForm.tsx component: import MyForm from './MyForm' ; export default MyForm ;","title":"myForm/index.ts"},{"location":"development/connectors/","text":"Connector Development \u00b6 This section contains some information regarding the development of connectors. If you're interested in developing your own connector library, check out the Custom Connector page. The rest of the content in this section refers to the development of official connector libraries, which are: freenalytics-connector-web","title":"Introduction"},{"location":"development/connectors/#connector-development","text":"This section contains some information regarding the development of connectors. If you're interested in developing your own connector library, check out the Custom Connector page. The rest of the content in this section refers to the development of official connector libraries, which are: freenalytics-connector-web","title":"Connector Development"},{"location":"development/connectors/custom-connector/","text":"Custom Connector Development \u00b6 Connectors are a way to share a common interface that can integrate your particular projects with Freenalytics. Connectors generally depend on the schema and the type of application you're trying to connect. This section will guide you through the development process of creating your own connector library. Naming \u00b6 For the sake of ease of finding your connector library, it is recommended that your library's name begins with freenalytics-connector- . (i.e freenalytics-connector-web ) This is obviously not mandatory, but it is recommended. Define a Schema \u00b6 Since connectors depend on a schema , you should first define the structure of the data that you plan to retrieve and save in your Freenalytics application. As you may know, schemas are defined using JSONSchema . As a requirement, these schemas must be of type object at the top level. An example of what a schema could look like: type : object properties : command_name : type : string command_author : type : string command_success : type : boolean command_error_message : type : string server_count : type : integer member_count : type : integer Note This is the schema used by the Discord Bot Example . Creating the Connector Client \u00b6 Connector clients are just an interface that integrates to Freenalytics through HTTP(s) API calls. You're free to use any language of your choice. Required Parameters \u00b6 Your connector client may have multiple parameters that may adjust its functionality that could be specified by a developer using your library. However, there are some parameters that your client should take into account: apiUrl : The developer should be able to specify the URL of their Freenalytics API instance. domain : The developer should also be able to include the application domain to use for their own project. Uploading Data \u00b6 In order to upload data payloads, your client should make an HTTP POST request with a application/json body containing the data payload to the following address: $API_URL$/applications/$DOMAIN$/data The shape of the payload may differ across different types of requests. After all, your schema may not necessarily describe data with entries that are interdependent with each other. Additional Considerations \u00b6 It is generally expected that connector libraries serve as some sort of automatic data upload. The developer shouldn't need to \"think\" about when to upload the analytics data, it is up to the connector to try and upload this data automatically, generally through the use of events. Your connector may expose data upload methods to allow the developer to explicitly to upload something. However, in the case that your connector library is able to register an event handler that can automate data upload, it should be the preferred way. Example \u00b6 As an example of what a connector client may look like, the code below includes the client used by the Discord Bot Example which integrates with the schema displayed above. In this case, the client has 3 parameters that allow the developer to customize the behavior of the client: options.apiUrl to specify to what Freenalytics instance the data will go to. options.domain to specify to what Freenalytics application the data will go to. options.interval to specify a polling interval that the project will use to upload the server_count and member_count variables. Next, the client exposes a initialize() method that registers event handlers and interval jobs that will automatically upload the relevant data to the Freenalytics instance. In this case, the client object (which is a ExtendedClient from @greencoast/discord.js-extended ) is the main client that represents the Discord bot, it is an EventEmitter that emits events whenever a command is issued and whenever it runs into an error. Using these events, the connector client can easily register handlers for these events that can automatically upload data regarding command execution by the users of the bot, through the handleCommandExecution() and handleCommandError() methods. Additionally, the postIntervalHandler() method calculates the number of servers and members that the bot has access to and uploads this data automatically. With this in mind, the developer that wants to use this connector client will only need to instantiate and initialize it as such: const client = new ExtendedClient (); // This is the @greencoast/discord.js-extended Client. It's functionality is irrelevant to the example. client . analytics = new FreenalyticsClient ( client , { apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpuvshlb8i8mb1' , interval : 10000 }); client . analytics . initialize (); And that's it. The developer can now \"forget\" that this analytics client exists.","title":"Custom Connector"},{"location":"development/connectors/custom-connector/#custom-connector-development","text":"Connectors are a way to share a common interface that can integrate your particular projects with Freenalytics. Connectors generally depend on the schema and the type of application you're trying to connect. This section will guide you through the development process of creating your own connector library.","title":"Custom Connector Development"},{"location":"development/connectors/custom-connector/#naming","text":"For the sake of ease of finding your connector library, it is recommended that your library's name begins with freenalytics-connector- . (i.e freenalytics-connector-web ) This is obviously not mandatory, but it is recommended.","title":"Naming"},{"location":"development/connectors/custom-connector/#define-a-schema","text":"Since connectors depend on a schema , you should first define the structure of the data that you plan to retrieve and save in your Freenalytics application. As you may know, schemas are defined using JSONSchema . As a requirement, these schemas must be of type object at the top level. An example of what a schema could look like: type : object properties : command_name : type : string command_author : type : string command_success : type : boolean command_error_message : type : string server_count : type : integer member_count : type : integer Note This is the schema used by the Discord Bot Example .","title":"Define a Schema"},{"location":"development/connectors/custom-connector/#creating-the-connector-client","text":"Connector clients are just an interface that integrates to Freenalytics through HTTP(s) API calls. You're free to use any language of your choice.","title":"Creating the Connector Client"},{"location":"development/connectors/custom-connector/#required-parameters","text":"Your connector client may have multiple parameters that may adjust its functionality that could be specified by a developer using your library. However, there are some parameters that your client should take into account: apiUrl : The developer should be able to specify the URL of their Freenalytics API instance. domain : The developer should also be able to include the application domain to use for their own project.","title":"Required Parameters"},{"location":"development/connectors/custom-connector/#uploading-data","text":"In order to upload data payloads, your client should make an HTTP POST request with a application/json body containing the data payload to the following address: $API_URL$/applications/$DOMAIN$/data The shape of the payload may differ across different types of requests. After all, your schema may not necessarily describe data with entries that are interdependent with each other.","title":"Uploading Data"},{"location":"development/connectors/custom-connector/#additional-considerations","text":"It is generally expected that connector libraries serve as some sort of automatic data upload. The developer shouldn't need to \"think\" about when to upload the analytics data, it is up to the connector to try and upload this data automatically, generally through the use of events. Your connector may expose data upload methods to allow the developer to explicitly to upload something. However, in the case that your connector library is able to register an event handler that can automate data upload, it should be the preferred way.","title":"Additional Considerations"},{"location":"development/connectors/custom-connector/#example","text":"As an example of what a connector client may look like, the code below includes the client used by the Discord Bot Example which integrates with the schema displayed above. In this case, the client has 3 parameters that allow the developer to customize the behavior of the client: options.apiUrl to specify to what Freenalytics instance the data will go to. options.domain to specify to what Freenalytics application the data will go to. options.interval to specify a polling interval that the project will use to upload the server_count and member_count variables. Next, the client exposes a initialize() method that registers event handlers and interval jobs that will automatically upload the relevant data to the Freenalytics instance. In this case, the client object (which is a ExtendedClient from @greencoast/discord.js-extended ) is the main client that represents the Discord bot, it is an EventEmitter that emits events whenever a command is issued and whenever it runs into an error. Using these events, the connector client can easily register handlers for these events that can automatically upload data regarding command execution by the users of the bot, through the handleCommandExecution() and handleCommandError() methods. Additionally, the postIntervalHandler() method calculates the number of servers and members that the bot has access to and uploads this data automatically. With this in mind, the developer that wants to use this connector client will only need to instantiate and initialize it as such: const client = new ExtendedClient (); // This is the @greencoast/discord.js-extended Client. It's functionality is irrelevant to the example. client . analytics = new FreenalyticsClient ( client , { apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpuvshlb8i8mb1' , interval : 10000 }); client . analytics . initialize (); And that's it. The developer can now \"forget\" that this analytics client exists.","title":"Example"},{"location":"development/connectors/freenalytics-connector-web/","text":"freenalytics-connector-web \u00b6 Info This page refers to the development of the freenalytics-connector-web connector library. If you're looking for the usage of this library, head over to the Official Web Template page. The freenalytics-connector-web is an official connector library for webpages that integrate with the Web Template . Requirements \u00b6 In order to develop for this connector library you will need: git Node.js (Version 16.15.1 was used) Setting Up \u00b6 First, clone the repository: git clone https://github.com/freenalytics/freenalytics-connector-web And then, install the dependencies: npm install Starting the Development Server \u00b6 Since this library was made with TypeScript it needs to be bundled and served through a CDN. In order to manually test functionality, you may start the bundler in watch mode (will re-bundle the project on file save) with: npm run build:watch And then serve the contents of the dist folder with: npm run dev:serve This will start an HTTP server on port 9000 . Manually Testing \u00b6 In order to check functionality while developing the library, you may need to create a dummy HTML page that includes the library with the following tags: < script type = \"text/javascript\" src = \"http://localhost:9000/connector.min.js\" ></ script > < script type = \"text/javascript\" src = \"/analytics.js\" ></ script > Where the analytics.js file looks like this: const client = new freenalytics . Client ({ apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpu34tlb7s7mro' }); client . initialize (); Note You can use your own Freenalytics instance. In this case http://localhost:4000/api is just for example purposes. It may as well be https://analytics.me.com/api or whatever the URL for your Freenalytics instance may be. You can then use any HTTP server to serve this dummy HTML page (such as the Live Server extension for VSCode). Any code change will update the files that the HTTP server on port 9000 serves. However, you might need to hard refresh (Ctrl+Shift+R or Cmd+Shift+R) the webpage in case your browser has cached the script. Linting \u00b6 This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix Unit Testing \u00b6 This project contains unit tests that try to cover the code as much as possible. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch","title":"freenalytics-connector-web"},{"location":"development/connectors/freenalytics-connector-web/#freenalytics-connector-web","text":"Info This page refers to the development of the freenalytics-connector-web connector library. If you're looking for the usage of this library, head over to the Official Web Template page. The freenalytics-connector-web is an official connector library for webpages that integrate with the Web Template .","title":"freenalytics-connector-web"},{"location":"development/connectors/freenalytics-connector-web/#requirements","text":"In order to develop for this connector library you will need: git Node.js (Version 16.15.1 was used)","title":"Requirements"},{"location":"development/connectors/freenalytics-connector-web/#setting-up","text":"First, clone the repository: git clone https://github.com/freenalytics/freenalytics-connector-web And then, install the dependencies: npm install","title":"Setting Up"},{"location":"development/connectors/freenalytics-connector-web/#starting-the-development-server","text":"Since this library was made with TypeScript it needs to be bundled and served through a CDN. In order to manually test functionality, you may start the bundler in watch mode (will re-bundle the project on file save) with: npm run build:watch And then serve the contents of the dist folder with: npm run dev:serve This will start an HTTP server on port 9000 .","title":"Starting the Development Server"},{"location":"development/connectors/freenalytics-connector-web/#manually-testing","text":"In order to check functionality while developing the library, you may need to create a dummy HTML page that includes the library with the following tags: < script type = \"text/javascript\" src = \"http://localhost:9000/connector.min.js\" ></ script > < script type = \"text/javascript\" src = \"/analytics.js\" ></ script > Where the analytics.js file looks like this: const client = new freenalytics . Client ({ apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpu34tlb7s7mro' }); client . initialize (); Note You can use your own Freenalytics instance. In this case http://localhost:4000/api is just for example purposes. It may as well be https://analytics.me.com/api or whatever the URL for your Freenalytics instance may be. You can then use any HTTP server to serve this dummy HTML page (such as the Live Server extension for VSCode). Any code change will update the files that the HTTP server on port 9000 serves. However, you might need to hard refresh (Ctrl+Shift+R or Cmd+Shift+R) the webpage in case your browser has cached the script.","title":"Manually Testing"},{"location":"development/connectors/freenalytics-connector-web/#linting","text":"This project uses ESLint rules to maintain a consistent code style. You can run the linter to check for any linting errors with: npm run lint And fix any fixable errors automatically with: npm run lint:fix","title":"Linting"},{"location":"development/connectors/freenalytics-connector-web/#unit-testing","text":"This project contains unit tests that try to cover the code as much as possible. You can run the unit tests with: npm run test Or, if you want to run the test suites in watch mode (will re-run relevant tests on file save), you can use: npm run test:watch","title":"Unit Testing"},{"location":"examples/","text":"Examples \u00b6 This section contains some examples that have been created for the sake of illustrating the usage and integration of Freenalytics with your own projects. Currently there are 2 example applications: Webpage Example Discord Bot Example The idea behind these applications is to show you how you can integrate Freenalytics with any type of application that may benefit from keeping track of usage data.","title":"Introduction"},{"location":"examples/#examples","text":"This section contains some examples that have been created for the sake of illustrating the usage and integration of Freenalytics with your own projects. Currently there are 2 example applications: Webpage Example Discord Bot Example The idea behind these applications is to show you how you can integrate Freenalytics with any type of application that may benefit from keeping track of usage data.","title":"Examples"},{"location":"examples/discord-bot-example/","text":"Discord Bot Example \u00b6 The freenalytics/example-discord-bot repository includes a small bot for Discord that contains two simple commands that greet the user and that makes the command run into an error with an 80% chance. There is no official template for Discord bots, however, the Freenalytics application that was created uses the following schema: type : object properties : command_name : type : string command_author : type : string command_success : type : boolean command_error_message : type : string server_count : type : integer member_count : type : integer The bot will then keep track of the name of the command that was issued, the username of who issued the command, whether it was successful, and if not, the error of the command. Additionally, it records the number of servers and users with access to the bot every 10 seconds. Client Implementation \u00b6 In case you're curious, here's the implementation of the client that communicates with Freenalytics. Dashboard Screenshot \u00b6 The Dashboard that you can expect to see is:","title":"Discord Bot Example"},{"location":"examples/discord-bot-example/#discord-bot-example","text":"The freenalytics/example-discord-bot repository includes a small bot for Discord that contains two simple commands that greet the user and that makes the command run into an error with an 80% chance. There is no official template for Discord bots, however, the Freenalytics application that was created uses the following schema: type : object properties : command_name : type : string command_author : type : string command_success : type : boolean command_error_message : type : string server_count : type : integer member_count : type : integer The bot will then keep track of the name of the command that was issued, the username of who issued the command, whether it was successful, and if not, the error of the command. Additionally, it records the number of servers and users with access to the bot every 10 seconds.","title":"Discord Bot Example"},{"location":"examples/discord-bot-example/#client-implementation","text":"In case you're curious, here's the implementation of the client that communicates with Freenalytics.","title":"Client Implementation"},{"location":"examples/discord-bot-example/#dashboard-screenshot","text":"The Dashboard that you can expect to see is:","title":"Dashboard Screenshot"},{"location":"examples/webpage-example/","text":"Webpage Example \u00b6 The freenalytics/example-web repository includes a small website that displays a fake article. The website makes use of the Official Web Template and uploads some basic information of the current's user interactions, such as element clicks, referral sites, current routes, whether a scroll event has been fired, and so on. Dashboard Screenshot \u00b6 The Dashboard that you can expect to see is:","title":"Webpage Example"},{"location":"examples/webpage-example/#webpage-example","text":"The freenalytics/example-web repository includes a small website that displays a fake article. The website makes use of the Official Web Template and uploads some basic information of the current's user interactions, such as element clicks, referral sites, current routes, whether a scroll event has been fired, and so on.","title":"Webpage Example"},{"location":"examples/webpage-example/#dashboard-screenshot","text":"The Dashboard that you can expect to see is:","title":"Dashboard Screenshot"},{"location":"getting-started/","text":"Getting Started \u00b6 This section contains some information to help you get up and running with Freenalytics. Installation \u00b6 To check how to install Freenalytics, check out Hosting with Docker which is the recommended way to host this service. If you wish to install this manually without Docker , check out Hosting with Node.js . Hosting this manually will require you to build the service on your machine. It also assumes you have existing MongoDB and Redis instances to attach to Freenalytics. Using Freenalytics \u00b6 Once you're all set up, check out: Creating your First Application Uploading Data To learn how to use this service and integrate it to your applications. In case you need to export all your data as a CSV file, check out Export Data .","title":"Introduction"},{"location":"getting-started/#getting-started","text":"This section contains some information to help you get up and running with Freenalytics.","title":"Getting Started"},{"location":"getting-started/#installation","text":"To check how to install Freenalytics, check out Hosting with Docker which is the recommended way to host this service. If you wish to install this manually without Docker , check out Hosting with Node.js . Hosting this manually will require you to build the service on your machine. It also assumes you have existing MongoDB and Redis instances to attach to Freenalytics.","title":"Installation"},{"location":"getting-started/#using-freenalytics","text":"Once you're all set up, check out: Creating your First Application Uploading Data To learn how to use this service and integrate it to your applications. In case you need to export all your data as a CSV file, check out Export Data .","title":"Using Freenalytics"},{"location":"getting-started/creating-your-first-application/","text":"Creating your First Application \u00b6 Creating an Application \u00b6 First, login to your web dashboard and inside the main page, click in Create New . Inside the application form, fill out the required fields with the information of your application. The Template Schema field should be a valid JSON Schema which will be used to structure and validate any incoming data entry. Schemas should be of type object at the top level and must include any data fields as properties. In the case that the structure contains data that is independent from each other you should probably not set those properties as required. Keep in Mind Schemas cannot be updated. If you need to change the schema of the application you will need to create a new application with the new schema. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. Example Above \u00b6 In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" } If there is an error, the bot should upload a payload like: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" , \"command_error\" : \"Tried to divide by 0.\" } And daily on a cronjob basis, the bot could upload a payload like: { \"daily_server_count\" : 42 }","title":"Creating your First Application"},{"location":"getting-started/creating-your-first-application/#creating-your-first-application","text":"","title":"Creating your First Application"},{"location":"getting-started/creating-your-first-application/#creating-an-application","text":"First, login to your web dashboard and inside the main page, click in Create New . Inside the application form, fill out the required fields with the information of your application. The Template Schema field should be a valid JSON Schema which will be used to structure and validate any incoming data entry. Schemas should be of type object at the top level and must include any data fields as properties. In the case that the structure contains data that is independent from each other you should probably not set those properties as required. Keep in Mind Schemas cannot be updated. If you need to change the schema of the application you will need to create a new application with the new schema. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields.","title":"Creating an Application"},{"location":"getting-started/creating-your-first-application/#example-above","text":"In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" } If there is an error, the bot should upload a payload like: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" , \"command_error\" : \"Tried to divide by 0.\" } And daily on a cronjob basis, the bot could upload a payload like: { \"daily_server_count\" : 42 }","title":"Example Above"},{"location":"getting-started/docker-hosting/","text":"Hosting with Docker \u00b6 The recommended way to host this application is through Docker . Docker Compose \u00b6 You can base yourself off of this docker-compose.yml file: version : '3.9' services : freenalytics : image : ghcr.io/freenalytics/freenalytics:latest restart : unless-stopped depends_on : - mongo - redis ports : - '3000:3000' environment : MONGODB_URI : mongodb://root:password@mongo:27017/freenalytics?authSource=admin REDIS_URI : redis://redis:6379 JWT_SECRET : MY_SUPER_SECRET REGISTRATION_OPEN : true mongo : image : mongo:latest restart : unless-stopped volumes : - ./data-mongo:/data/db environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password redis : image : redis:latest restart : unless-stopped volumes : - ./data-redis:/data command : redis-server --loglevel warning You can then start the service by running the following in the same folder where the docker-compose.yml file is located: docker-compose up -d This will start the Freenalytics server with all the required services to run. The web dashboard will be available at http://localhost:3000 . Configuration \u00b6 You can configure the service with the following environment variables: Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. HTTPS \u00b6 The application does not run through HTTPS by itself, it is recommended to use a reverse proxy such as nginx or use something like Cloudflare to expose the service through HTTPS.","title":"Hosting with Docker"},{"location":"getting-started/docker-hosting/#hosting-with-docker","text":"The recommended way to host this application is through Docker .","title":"Hosting with Docker"},{"location":"getting-started/docker-hosting/#docker-compose","text":"You can base yourself off of this docker-compose.yml file: version : '3.9' services : freenalytics : image : ghcr.io/freenalytics/freenalytics:latest restart : unless-stopped depends_on : - mongo - redis ports : - '3000:3000' environment : MONGODB_URI : mongodb://root:password@mongo:27017/freenalytics?authSource=admin REDIS_URI : redis://redis:6379 JWT_SECRET : MY_SUPER_SECRET REGISTRATION_OPEN : true mongo : image : mongo:latest restart : unless-stopped volumes : - ./data-mongo:/data/db environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password redis : image : redis:latest restart : unless-stopped volumes : - ./data-redis:/data command : redis-server --loglevel warning You can then start the service by running the following in the same folder where the docker-compose.yml file is located: docker-compose up -d This will start the Freenalytics server with all the required services to run. The web dashboard will be available at http://localhost:3000 .","title":"Docker Compose"},{"location":"getting-started/docker-hosting/#configuration","text":"You can configure the service with the following environment variables: Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created.","title":"Configuration"},{"location":"getting-started/docker-hosting/#https","text":"The application does not run through HTTPS by itself, it is recommended to use a reverse proxy such as nginx or use something like Cloudflare to expose the service through HTTPS.","title":"HTTPS"},{"location":"getting-started/exporting-data/","text":"Exporting Data \u00b6 Inside your application dashboard, clicking on the Data Entries sidebar item will take you to a page that will display a table with all the data entries that there is for your application. In case you need export your data entries as a CSV for processing, you can click on the Export everything as CSV button which will download a CSV with everything for this application.","title":"Exporting Data"},{"location":"getting-started/exporting-data/#exporting-data","text":"Inside your application dashboard, clicking on the Data Entries sidebar item will take you to a page that will display a table with all the data entries that there is for your application. In case you need export your data entries as a CSV for processing, you can click on the Export everything as CSV button which will download a CSV with everything for this application.","title":"Exporting Data"},{"location":"getting-started/node-hosting/","text":"Manual Hosting with Node.js \u00b6 The recommended way to host this application is through Docker , follow this guide if for some reason you do not want to use Docker. Keep in mind that this guide assumes you have a MongoDB and Redis instances ready to use. Installation \u00b6 Pre requirements \u00b6 First, make sure you're running at least Node v16.15.1 . Clone this repository: git clone https://github.com/freenalytics/freenalytics Inside the web-dashboard folder \u00b6 Install the dependencies: npm ci And build the application: npm run build This will create a folder named build . You need to move this folder into the server folder and rename it to client-build . mv build ../server/client-build Inside the server folder \u00b6 Install the dependencies: npm ci And build the server: npm run build Create a file named .env and add the following configuration: MONGODB_URI= REDIS_URI= JWT_SECRET= JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true PORT=3000 Configuration \u00b6 Here's a description of each of the configuration variables. Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. Info Add the URIs for your MongoDB and Redis instances in their respective variables. Keep in Mind Make sure you have moved the web-dashboard/build folder into server/client-build . You can now start the server with: npm start The server will be listening to the port you've specified above. Auto-starting \u00b6 If you're running a Linux based operating system, you may use something like systemd to create an autostart service for this application. You can base yourself off the following service file. [Unit] Description=Freenalytics Service After=network.target [Service] Type=simple User=<USER> WorkingDirectory=<SERVER_LOCATION> ExecStart=/usr/bin/npm start Restart=always [Install] WantedBy=multi-user.target Note Replace <USER> with your Unix username and <SERVER_LOCATION> with the directory where the server is located (the server folder). You can save this file in: sudo nano /etc/systemd/system/freenalytics.service And enable it with: sudo systemctl start freenalytics.service sudo systemctl enable freenalytics.service","title":"Hosting with Node.js"},{"location":"getting-started/node-hosting/#manual-hosting-with-nodejs","text":"The recommended way to host this application is through Docker , follow this guide if for some reason you do not want to use Docker. Keep in mind that this guide assumes you have a MongoDB and Redis instances ready to use.","title":"Manual Hosting with Node.js"},{"location":"getting-started/node-hosting/#installation","text":"","title":"Installation"},{"location":"getting-started/node-hosting/#pre-requirements","text":"First, make sure you're running at least Node v16.15.1 . Clone this repository: git clone https://github.com/freenalytics/freenalytics","title":"Pre requirements"},{"location":"getting-started/node-hosting/#inside-the-web-dashboard-folder","text":"Install the dependencies: npm ci And build the application: npm run build This will create a folder named build . You need to move this folder into the server folder and rename it to client-build . mv build ../server/client-build","title":"Inside the web-dashboard folder"},{"location":"getting-started/node-hosting/#inside-the-server-folder","text":"Install the dependencies: npm ci And build the server: npm run build Create a file named .env and add the following configuration: MONGODB_URI= REDIS_URI= JWT_SECRET= JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true PORT=3000","title":"Inside the server folder"},{"location":"getting-started/node-hosting/#configuration","text":"Here's a description of each of the configuration variables. Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. Info Add the URIs for your MongoDB and Redis instances in their respective variables. Keep in Mind Make sure you have moved the web-dashboard/build folder into server/client-build . You can now start the server with: npm start The server will be listening to the port you've specified above.","title":"Configuration"},{"location":"getting-started/node-hosting/#auto-starting","text":"If you're running a Linux based operating system, you may use something like systemd to create an autostart service for this application. You can base yourself off the following service file. [Unit] Description=Freenalytics Service After=network.target [Service] Type=simple User=<USER> WorkingDirectory=<SERVER_LOCATION> ExecStart=/usr/bin/npm start Restart=always [Install] WantedBy=multi-user.target Note Replace <USER> with your Unix username and <SERVER_LOCATION> with the directory where the server is located (the server folder). You can save this file in: sudo nano /etc/systemd/system/freenalytics.service And enable it with: sudo systemctl start freenalytics.service sudo systemctl enable freenalytics.service","title":"Auto-starting"},{"location":"getting-started/uploading-data/","text":"Uploading Data \u00b6 Some Information \u00b6 Inside your application dashboard, clicking on the Some Information sidebar item will take you to a page that will give you some insight on how to upload data for your application. This page will remind you of the application schema and will show you the endpoint to upload data with a little example of what type of data you can upload. Example \u00b6 In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number And the endpoint is: POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) If there is an error, the bot should upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\", \"command_error\": \"Tried to divide by 0.\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' , command_error : 'Tried to divide by 0.' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' , 'command_error' : 'Tried to divide by 0.' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) And daily on a cronjob basis, the bot could upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"daily_server_count\": 42 }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { daily_server_count : 42 }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'daily_server_count' : 42 } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response )","title":"Uploading Data"},{"location":"getting-started/uploading-data/#uploading-data","text":"","title":"Uploading Data"},{"location":"getting-started/uploading-data/#some-information","text":"Inside your application dashboard, clicking on the Some Information sidebar item will take you to a page that will give you some insight on how to upload data for your application. This page will remind you of the application schema and will show you the endpoint to upload data with a little example of what type of data you can upload.","title":"Some Information"},{"location":"getting-started/uploading-data/#example","text":"In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number And the endpoint is: POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) If there is an error, the bot should upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\", \"command_error\": \"Tried to divide by 0.\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' , command_error : 'Tried to divide by 0.' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' , 'command_error' : 'Tried to divide by 0.' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) And daily on a cronjob basis, the bot could upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"daily_server_count\": 42 }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { daily_server_count : 42 }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'daily_server_count' : 42 } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response )","title":"Example"},{"location":"official-templates/","text":"Official Templates \u00b6 This section contains some information regarding the existing official templates. An official template is a solution for common uses of the platform. These templates include already defined schemas and connectors for your application to allow you to quickly create an application and start uploading data to it. Currently, every official template available is documented on this section.","title":"Introduction"},{"location":"official-templates/#official-templates","text":"This section contains some information regarding the existing official templates. An official template is a solution for common uses of the platform. These templates include already defined schemas and connectors for your application to allow you to quickly create an application and start uploading data to it. Currently, every official template available is documented on this section.","title":"Official Templates"},{"location":"official-templates/web-template/","text":"Official Web Template \u00b6 The official web template allows you to integrate Freenalytics into your web project. Usage \u00b6 In order to use this connector, you need to create a new application on your Freenalytics instance with the Web Template option on the create form. This template will create a new application with the following schema: type : object properties : page_title : type : string url_route : type : string user_time_in_page : type : number user_scrolled : type : boolean user_first_visit : type : boolean user_location : type : string referrer : type : string num_of_clicks : type : integer element_clicked : type : object properties : url_route : type : string tag_name : type : string class_name : type : string id : type : string page_x : type : integer page_y : type : integer page_width : type : integer page_height : type : integer client_x : type : integer client_y : type : integer client_width : type : integer client_height : type : integer Once you have your new application set up, you should include the following script tags inside your head tag in your html page. < script type = \"text/javascript\" src = \"https://cdn.jsdelivr.net/gh/freenalytics/freenalytics-connector-web@v1.1.0/connector.min.js\" ></ script > < script type = \"text/javascript\" src = \"/analytics.js\" ></ script > Notice that the second script tag includes a local script. This script should be created by on your end in order to connect the connector library to your own Freenalytics instance. In this case, the analytics.js file could look like: const client = new freenalytics . Client ({ apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpu34tlb7s7mro' }); client . initialize (); Note Make sure to replace the apiUrl with your actual Freenalytics's API URL (must end with /api ). Same goes with the domain which corresponds to your application's domain. Once set, the connector will send the relevant information from the client visiting your website periodically. Example Application \u00b6 An example application that uses this library has been built as an illustration. For more information, check out the Webpage example section. Development \u00b6 If you're interested in developing this library, head over to the Web connector development guide.","title":"Official Web Template"},{"location":"official-templates/web-template/#official-web-template","text":"The official web template allows you to integrate Freenalytics into your web project.","title":"Official Web Template"},{"location":"official-templates/web-template/#usage","text":"In order to use this connector, you need to create a new application on your Freenalytics instance with the Web Template option on the create form. This template will create a new application with the following schema: type : object properties : page_title : type : string url_route : type : string user_time_in_page : type : number user_scrolled : type : boolean user_first_visit : type : boolean user_location : type : string referrer : type : string num_of_clicks : type : integer element_clicked : type : object properties : url_route : type : string tag_name : type : string class_name : type : string id : type : string page_x : type : integer page_y : type : integer page_width : type : integer page_height : type : integer client_x : type : integer client_y : type : integer client_width : type : integer client_height : type : integer Once you have your new application set up, you should include the following script tags inside your head tag in your html page. < script type = \"text/javascript\" src = \"https://cdn.jsdelivr.net/gh/freenalytics/freenalytics-connector-web@v1.1.0/connector.min.js\" ></ script > < script type = \"text/javascript\" src = \"/analytics.js\" ></ script > Notice that the second script tag includes a local script. This script should be created by on your end in order to connect the connector library to your own Freenalytics instance. In this case, the analytics.js file could look like: const client = new freenalytics . Client ({ apiUrl : 'http://localhost:4000/api' , domain : 'FD-107hpu34tlb7s7mro' }); client . initialize (); Note Make sure to replace the apiUrl with your actual Freenalytics's API URL (must end with /api ). Same goes with the domain which corresponds to your application's domain. Once set, the connector will send the relevant information from the client visiting your website periodically.","title":"Usage"},{"location":"official-templates/web-template/#example-application","text":"An example application that uses this library has been built as an illustration. For more information, check out the Webpage example section.","title":"Example Application"},{"location":"official-templates/web-template/#development","text":"If you're interested in developing this library, head over to the Web connector development guide.","title":"Development"},{"location":"reference/","text":"Reference \u00b6 This section contains some reference information regarding the entities that compose this application. If you wish to interact with the platform programmatically through the API, check the API Reference .","title":"Introduction"},{"location":"reference/#reference","text":"This section contains some reference information regarding the entities that compose this application. If you wish to interact with the platform programmatically through the API, check the API Reference .","title":"Reference"},{"location":"reference/application/","text":"Application \u00b6 An application is a space in the platform for you to upload data entries that conform to the schema that you've defined when you created your application. An user can have any number of applications and an application has a non-modifiable schema that defines the structure of the data to upload. In the web dashboard, the main site contains a list of all the applications that the user owns. When clicking on an application you'll be taken to the dashboard for that application where you can find some auto generated graphs and previews for the uploaded data, a table with all the raw data entries and an option to export everything as a CSV for custom processing. Check out creating your first application to see how to create an application in the platform. When creating an application, you'll be given a domain ID (of the shape of FD-107hpu2306l9ui95bq ) that can be used to upload your data.","title":"Application"},{"location":"reference/application/#application","text":"An application is a space in the platform for you to upload data entries that conform to the schema that you've defined when you created your application. An user can have any number of applications and an application has a non-modifiable schema that defines the structure of the data to upload. In the web dashboard, the main site contains a list of all the applications that the user owns. When clicking on an application you'll be taken to the dashboard for that application where you can find some auto generated graphs and previews for the uploaded data, a table with all the raw data entries and an option to export everything as a CSV for custom processing. Check out creating your first application to see how to create an application in the platform. When creating an application, you'll be given a domain ID (of the shape of FD-107hpu2306l9ui95bq ) that can be used to upload your data.","title":"Application"},{"location":"reference/connector/","text":"Connector \u00b6 A connector is an easy way to include a pre-implemented data upload client in the information of an application. The idea behind this entity is to allow applications that share commonly used schemas to have information regarding the library that the developer can use to integrate with the platform directly instead of having to manually implement the client themselves. An application can have any number of connectors and a connector is composed of 2 things: language : The language of the connector library. package_url : The URL of the connector library to download. Official Templates make use of connectors to allow you to quickly create applications that share a common use.","title":"Connector"},{"location":"reference/connector/#connector","text":"A connector is an easy way to include a pre-implemented data upload client in the information of an application. The idea behind this entity is to allow applications that share commonly used schemas to have information regarding the library that the developer can use to integrate with the platform directly instead of having to manually implement the client themselves. An application can have any number of connectors and a connector is composed of 2 things: language : The language of the connector library. package_url : The URL of the connector library to download. Official Templates make use of connectors to allow you to quickly create applications that share a common use.","title":"Connector"},{"location":"reference/data-entries/","text":"Data Entries \u00b6 Every application exposes an endpoint on the server to upload data entries. Data entries need to conform to the schema defined in the application. Depending on the schema defined, you may or may not upload incomplete data. This is useful in the case where your schema includes data properties for which you might not be able to upload data at the same time. For example, in the example seen in creating your first application , for the following schema: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number The property daily_server_count is supposed to be uploaded once every day on a cronjob basis. For the case of the rest of the properties, they're supposed to be uploaded whenever a user executes a command, and whether this command errors or not it may or may not include the command_error property. Given this case, there will be rows for each data entry where not everything will have a value. For more information on how to upload data, check out uploading data .","title":"Data Entries"},{"location":"reference/data-entries/#data-entries","text":"Every application exposes an endpoint on the server to upload data entries. Data entries need to conform to the schema defined in the application. Depending on the schema defined, you may or may not upload incomplete data. This is useful in the case where your schema includes data properties for which you might not be able to upload data at the same time. For example, in the example seen in creating your first application , for the following schema: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number The property daily_server_count is supposed to be uploaded once every day on a cronjob basis. For the case of the rest of the properties, they're supposed to be uploaded whenever a user executes a command, and whether this command errors or not it may or may not include the command_error property. Given this case, there will be rows for each data entry where not everything will have a value. For more information on how to upload data, check out uploading data .","title":"Data Entries"},{"location":"reference/schema/","text":"Schema \u00b6 Every application has a schema that defines the structure of the data that will be uploaded. Apart from defining the structure of the data it is also used to validate it on upload. Schemas cannot be updated when an application is created. If you need to change the structure of the data for your application then you need to create a new application. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. A schema should be valid a JSON Schema written in YML. Every schema should be of type object and can have any type as properties. The server will validate data on upload with the flag additionalProperties set to false . If your schema contains this directive it will be overridden for the top level object. Some examples of valid schemas: type : object properties : name : type : string phone : type : number nested_data : type : object properties : title : type : string subtitle : type : string required : - title - subtitle arr_numbers : type : array items : type : number required : - name - phone Or, type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number","title":"Schema"},{"location":"reference/schema/#schema","text":"Every application has a schema that defines the structure of the data that will be uploaded. Apart from defining the structure of the data it is also used to validate it on upload. Schemas cannot be updated when an application is created. If you need to change the structure of the data for your application then you need to create a new application. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. A schema should be valid a JSON Schema written in YML. Every schema should be of type object and can have any type as properties. The server will validate data on upload with the flag additionalProperties set to false . If your schema contains this directive it will be overridden for the top level object. Some examples of valid schemas: type : object properties : name : type : string phone : type : number nested_data : type : object properties : title : type : string subtitle : type : string required : - title - subtitle arr_numbers : type : array items : type : number required : - name - phone Or, type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number","title":"Schema"}]}