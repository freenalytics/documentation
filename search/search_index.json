{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Info This site is still under construction. If you wish to track this project's progress check out the Trello board . An open source, self-hosted dashboard for usage analytics of general use applications. Author \u00b6 This application was made by Freenalytics as a final college project.","title":"Home"},{"location":"#home","text":"Info This site is still under construction. If you wish to track this project's progress check out the Trello board . An open source, self-hosted dashboard for usage analytics of general use applications.","title":"Home"},{"location":"#author","text":"This application was made by Freenalytics as a final college project.","title":"Author"},{"location":"development/","text":"Development \u00b6 This sections contains some information regarding the development process of this project. If you wish to contribute to the development of Freenalytics, check out the following pages: Web Dashboard Development Server Development","title":"Introduction"},{"location":"development/#development","text":"This sections contains some information regarding the development process of this project. If you wish to contribute to the development of Freenalytics, check out the following pages: Web Dashboard Development Server Development","title":"Development"},{"location":"development/server/","text":"Info Under construction. If you wish to track this project's progress check out the Trello board .","title":"Server"},{"location":"development/web-dashboard/","text":"Info Under construction. If you wish to track this project's progress check out the Trello board .","title":"Web Dashboard"},{"location":"getting-started/","text":"Getting Started \u00b6 This section contains some information to help you get up and running with Freenalytics. Installation \u00b6 To check how to install Freenalytics, check out Hosting with Docker which is the recommended way to host this service. If you wish to install this manually without Docker , check out Hosting with Node.js . Hosting this manually will require you to build the service on your machine. It also assumes you have existing MongoDB and Redis instances to attach to Freenalytics. Using Freenalytics \u00b6 Once you're all set up, check out: Creating your First Application Uploading Data To learn how to use this service and integrate it to your applications. In case you need to export all your data as a CSV file, check out Export Data .","title":"Introduction"},{"location":"getting-started/#getting-started","text":"This section contains some information to help you get up and running with Freenalytics.","title":"Getting Started"},{"location":"getting-started/#installation","text":"To check how to install Freenalytics, check out Hosting with Docker which is the recommended way to host this service. If you wish to install this manually without Docker , check out Hosting with Node.js . Hosting this manually will require you to build the service on your machine. It also assumes you have existing MongoDB and Redis instances to attach to Freenalytics.","title":"Installation"},{"location":"getting-started/#using-freenalytics","text":"Once you're all set up, check out: Creating your First Application Uploading Data To learn how to use this service and integrate it to your applications. In case you need to export all your data as a CSV file, check out Export Data .","title":"Using Freenalytics"},{"location":"getting-started/creating-your-first-application/","text":"Creating your First Application \u00b6 Creating an Application \u00b6 First, login to your web dashboard and inside the main page, click in Create New . Inside the application form, fill out the required fields with the information of your application. The Template Schema field should be a valid JSON Schema which will be used to structure and validate any incoming data entry. Schemas should be of type object at the top level and must include any data fields as properties. In the case that the structure contains data that is independent from each other you should probably not set those properties as required. Keep in Mind Schemas cannot be updated. If you need to change the schema of the application you will need to create a new application with the new schema. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. Example Above \u00b6 In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" } If there is an error, the bot should upload a payload like: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" , \"command_error\" : \"Tried to divide by 0.\" } And daily on a cronjob basis, the bot could upload a payload like: { \"daily_server_count\" : 42 }","title":"Creating your First Application"},{"location":"getting-started/creating-your-first-application/#creating-your-first-application","text":"","title":"Creating your First Application"},{"location":"getting-started/creating-your-first-application/#creating-an-application","text":"First, login to your web dashboard and inside the main page, click in Create New . Inside the application form, fill out the required fields with the information of your application. The Template Schema field should be a valid JSON Schema which will be used to structure and validate any incoming data entry. Schemas should be of type object at the top level and must include any data fields as properties. In the case that the structure contains data that is independent from each other you should probably not set those properties as required. Keep in Mind Schemas cannot be updated. If you need to change the schema of the application you will need to create a new application with the new schema. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields.","title":"Creating an Application"},{"location":"getting-started/creating-your-first-application/#example-above","text":"In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" } If there is an error, the bot should upload a payload like: { \"command_executed\" : \"my_command\" , \"command_author\" : \"username\" , \"command_error\" : \"Tried to divide by 0.\" } And daily on a cronjob basis, the bot could upload a payload like: { \"daily_server_count\" : 42 }","title":"Example Above"},{"location":"getting-started/docker-hosting/","text":"Hosting with Docker \u00b6 The recommended way to host this application is through Docker . Docker Compose \u00b6 You can base yourself off of this docker-compose.yml file: version : '3.9' services : freenalytics : image : ghcr.io/freenalytics/freenalytics:latest restart : unless-stopped depends_on : - mongo - redis ports : - '3000:3000' environment : MONGODB_URI : mongodb://root:password@mongo:27017/freenalytics?authSource=admin REDIS_URI : redis://redis:6379 JWT_SECRET : MY_SUPER_SECRET REGISTRATION_OPEN : true mongo : image : mongo:latest restart : unless-stopped volumes : - ./data-mongo:/data/db environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password redis : image : redis:latest restart : unless-stopped volumes : - ./data-redis:/data command : redis-server --loglevel warning You can then start the service by running the following in the same folder where the docker-compose.yml file is located: docker-compose up -d This will start the Freenalytics server with all the required services to run. The web dashboard will be available at http://localhost:3000 . Configuration \u00b6 You can configure the service with the following environment variables: Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. HTTPS \u00b6 The application does not run through HTTPS by itself, it is recommended to use a reverse proxy such as nginx or use something like Cloudflare to expose the service through HTTPS.","title":"Hosting with Docker"},{"location":"getting-started/docker-hosting/#hosting-with-docker","text":"The recommended way to host this application is through Docker .","title":"Hosting with Docker"},{"location":"getting-started/docker-hosting/#docker-compose","text":"You can base yourself off of this docker-compose.yml file: version : '3.9' services : freenalytics : image : ghcr.io/freenalytics/freenalytics:latest restart : unless-stopped depends_on : - mongo - redis ports : - '3000:3000' environment : MONGODB_URI : mongodb://root:password@mongo:27017/freenalytics?authSource=admin REDIS_URI : redis://redis:6379 JWT_SECRET : MY_SUPER_SECRET REGISTRATION_OPEN : true mongo : image : mongo:latest restart : unless-stopped volumes : - ./data-mongo:/data/db environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password redis : image : redis:latest restart : unless-stopped volumes : - ./data-redis:/data command : redis-server --loglevel warning You can then start the service by running the following in the same folder where the docker-compose.yml file is located: docker-compose up -d This will start the Freenalytics server with all the required services to run. The web dashboard will be available at http://localhost:3000 .","title":"Docker Compose"},{"location":"getting-started/docker-hosting/#configuration","text":"You can configure the service with the following environment variables: Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created.","title":"Configuration"},{"location":"getting-started/docker-hosting/#https","text":"The application does not run through HTTPS by itself, it is recommended to use a reverse proxy such as nginx or use something like Cloudflare to expose the service through HTTPS.","title":"HTTPS"},{"location":"getting-started/exporting-data/","text":"Exporting Data \u00b6 Inside your application dashboard, clicking on the Data Entries sidebar item will take you to a page that will display a table with all the data entries that there is for your application. In case you need export your data entries as a CSV for processing, you can click on the Export everything as CSV button which will download a CSV with everything for this application.","title":"Exporting Data"},{"location":"getting-started/exporting-data/#exporting-data","text":"Inside your application dashboard, clicking on the Data Entries sidebar item will take you to a page that will display a table with all the data entries that there is for your application. In case you need export your data entries as a CSV for processing, you can click on the Export everything as CSV button which will download a CSV with everything for this application.","title":"Exporting Data"},{"location":"getting-started/node-hosting/","text":"Manual Hosting with Node.js \u00b6 The recommended way to host this application is through Docker , follow this guide if for some reason you do not want to use Docker. Keep in mind that this guide assumes you have a MongoDB and Redis instances ready to use. Installation \u00b6 Pre requirements \u00b6 First, make sure you're running at least Node v16.15.1 . Clone this repository: git clone https://github.com/freenalytics/freenalytics Inside the web-dashboard folder \u00b6 Install the dependencies: npm ci And build the application: npm run build This will create a folder named build . You need to move this folder into the server folder and rename it to client-build . mv build ../server/client-build Inside the server folder \u00b6 Install the dependencies: npm ci And build the server: npm run build Create a file named .env and add the following configuration: MONGODB_URI= REDIS_URI= JWT_SECRET= JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true PORT=3000 Configuration \u00b6 Here's a description of each of the configuration variables. Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. Info Add the URIs for your MongoDB and Redis instances in their respective variables. Keep in Mind Make sure you have moved the web-dashboard/build folder into server/client-build . You can now start the server with: npm start The server will be listening to the port you've specified above. Auto-starting \u00b6 If you're running a Linux based operating system, you may use something like systemd to create an autostart service for this application. You can base yourself off the following service file. [Unit] Description=Freenalytics Service After=network.target [Service] Type=simple User=<USER> WorkingDirectory=<SERVER_LOCATION> ExecStart=/usr/bin/npm start Restart=always [Install] WantedBy=multi-user.target Note Replace <USER> with your Unix username and <SERVER_LOCATION> with the directory where the server is located (the server folder). You can save this file in: sudo nano /etc/systemd/system/freenalytics.service And enable it with: sudo systemctl start freenalytics.service sudo systemctl enable freenalytics.service","title":"Hosting with Node.js"},{"location":"getting-started/node-hosting/#manual-hosting-with-nodejs","text":"The recommended way to host this application is through Docker , follow this guide if for some reason you do not want to use Docker. Keep in mind that this guide assumes you have a MongoDB and Redis instances ready to use.","title":"Manual Hosting with Node.js"},{"location":"getting-started/node-hosting/#installation","text":"","title":"Installation"},{"location":"getting-started/node-hosting/#pre-requirements","text":"First, make sure you're running at least Node v16.15.1 . Clone this repository: git clone https://github.com/freenalytics/freenalytics","title":"Pre requirements"},{"location":"getting-started/node-hosting/#inside-the-web-dashboard-folder","text":"Install the dependencies: npm ci And build the application: npm run build This will create a folder named build . You need to move this folder into the server folder and rename it to client-build . mv build ../server/client-build","title":"Inside the web-dashboard folder"},{"location":"getting-started/node-hosting/#inside-the-server-folder","text":"Install the dependencies: npm ci And build the server: npm run build Create a file named .env and add the following configuration: MONGODB_URI= REDIS_URI= JWT_SECRET= JWT_TOKEN_DURATION=604800 REGISTRATION_OPEN=true PORT=3000","title":"Inside the server folder"},{"location":"getting-started/node-hosting/#configuration","text":"Here's a description of each of the configuration variables. Environment Variable Required Description MONGODB_URI Yes The URI of the MongoDB instance to use as database. REDIS_URI Yes The URI of the Redis instance to use as cache. JWT_SECRET Yes A string to use as secret to sign JWT tokens. You can use openssl rand -hex 32 to generate one for you. JWT_TOKEN_DURATION No The time (in seconds) that the user JWT tokens will last for. Defaults to 604800 which is 7 days. REGISTRATION_OPEN No Whether users can register to create an account. By default this is set to false . You can enable registration by setting this to true . User account registration is disabled by default. This makes sure that nobody else can create an account to abuse the service. It is recommended to keep this option set to false once all the necessary user accounts have been created. Info Add the URIs for your MongoDB and Redis instances in their respective variables. Keep in Mind Make sure you have moved the web-dashboard/build folder into server/client-build . You can now start the server with: npm start The server will be listening to the port you've specified above.","title":"Configuration"},{"location":"getting-started/node-hosting/#auto-starting","text":"If you're running a Linux based operating system, you may use something like systemd to create an autostart service for this application. You can base yourself off the following service file. [Unit] Description=Freenalytics Service After=network.target [Service] Type=simple User=<USER> WorkingDirectory=<SERVER_LOCATION> ExecStart=/usr/bin/npm start Restart=always [Install] WantedBy=multi-user.target Note Replace <USER> with your Unix username and <SERVER_LOCATION> with the directory where the server is located (the server folder). You can save this file in: sudo nano /etc/systemd/system/freenalytics.service And enable it with: sudo systemctl start freenalytics.service sudo systemctl enable freenalytics.service","title":"Auto-starting"},{"location":"getting-started/uploading-data/","text":"Uploading Data \u00b6 Some Information \u00b6 Inside your application dashboard, clicking on the Some Information sidebar item will take you to a page that will give you some insight on how to upload data for your application. This page will remind you of the application schema and will show you the endpoint to upload data with a little example of what type of data you can upload. Example \u00b6 In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number And the endpoint is: POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) If there is an error, the bot should upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\", \"command_error\": \"Tried to divide by 0.\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' , command_error : 'Tried to divide by 0.' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' , 'command_error' : 'Tried to divide by 0.' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) And daily on a cronjob basis, the bot could upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"daily_server_count\": 42 }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { daily_server_count : 42 }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'daily_server_count' : 42 } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response )","title":"Uploading Data"},{"location":"getting-started/uploading-data/#uploading-data","text":"","title":"Uploading Data"},{"location":"getting-started/uploading-data/#some-information","text":"Inside your application dashboard, clicking on the Some Information sidebar item will take you to a page that will give you some insight on how to upload data for your application. This page will remind you of the application schema and will show you the endpoint to upload data with a little example of what type of data you can upload.","title":"Some Information"},{"location":"getting-started/uploading-data/#example","text":"In the example above, the following schema is used: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number And the endpoint is: POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data This example is a basic schema for a Discord Bot . In this case each field means the following: command_error : Whenever a command throws any error, the bot will upload the error message. command_executed : This contains the name of the command that was executed. command_author : This contains the name of the user that executed the command. daily_server_count : This corresponds to the number of servers the bot is in. This value would be updated daily with a cronjob. In this case, in an ideal workflow, whenever a command is executed by a user, the bot should upload a payload with the following structure: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) If there is an error, the bot should upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"command_executed\": \"my_command\", \"command_author\": \"username\", \"command_error\": \"Tried to divide by 0.\" }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { command_executed : 'my_command' , command_author : 'username' , command_error : 'Tried to divide by 0.' }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'command_executed' : 'my_command' , 'command_author' : 'username' , 'command_error' : 'Tried to divide by 0.' } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response ) And daily on a cronjob basis, the bot could upload a payload like: Curl Node.js Python curl -X POST http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data \\ -H 'Content-Type: application/json' \\ -d '{ \"daily_server_count\": 42 }' const axios = require ( 'axios' ); axios . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , { daily_server_count : 42 }) . then (( response ) => console . log ( response . data )) . catch (( error ) => console . error ( error . response . data )); import requests payload = { 'daily_server_count' : 42 } response = requests . post ( 'http://localhost:4000/api/applications/FD-107hpu2306l9ui95bq/data' , json = payload ) print ( response )","title":"Example"},{"location":"official-templates/","text":"Official Templates \u00b6 This section contains some information regarding the existing official templates. An official template is a solution for common uses of the platform. These templates include already defined schemas and connectors for your application to allow you to quickly create an application and start uploading data to it. Currently, every official template available is documented on this section.","title":"Introduction"},{"location":"official-templates/#official-templates","text":"This section contains some information regarding the existing official templates. An official template is a solution for common uses of the platform. These templates include already defined schemas and connectors for your application to allow you to quickly create an application and start uploading data to it. Currently, every official template available is documented on this section.","title":"Official Templates"},{"location":"official-templates/web-template/","text":"Info Under construction. If you wish to track this project's progress check out the Trello board .","title":"Official Web Template"},{"location":"reference/","text":"Reference \u00b6 This section contains some reference information regarding the entities that compose this application. If you wish to interact with the platform programmatically through the API, check the API Reference .","title":"Introduction"},{"location":"reference/#reference","text":"This section contains some reference information regarding the entities that compose this application. If you wish to interact with the platform programmatically through the API, check the API Reference .","title":"Reference"},{"location":"reference/application/","text":"Application \u00b6 An application is a space in the platform for you to upload data entries that conform to the schema that you've defined when you created your application. An user can have any number of applications and an application has a non-modifiable schema that defines the structure of the data to upload. In the web dashboard, the main site contains a list of all the applications that the user owns. When clicking on an application you'll be taken to the dashboard for that application where you can find some auto generated graphs and previews for the uploaded data, a table with all the raw data entries and an option to export everything as a CSV for custom processing. Check out creating your first application to see how to create an application in the platform. When creating an application, you'll be given a domain ID (of the shape of FD-107hpu2306l9ui95bq ) that can be used to upload your data.","title":"Application"},{"location":"reference/application/#application","text":"An application is a space in the platform for you to upload data entries that conform to the schema that you've defined when you created your application. An user can have any number of applications and an application has a non-modifiable schema that defines the structure of the data to upload. In the web dashboard, the main site contains a list of all the applications that the user owns. When clicking on an application you'll be taken to the dashboard for that application where you can find some auto generated graphs and previews for the uploaded data, a table with all the raw data entries and an option to export everything as a CSV for custom processing. Check out creating your first application to see how to create an application in the platform. When creating an application, you'll be given a domain ID (of the shape of FD-107hpu2306l9ui95bq ) that can be used to upload your data.","title":"Application"},{"location":"reference/connector/","text":"Connector \u00b6 A connector is an easy way to include a pre-implemented data upload client in the information of an application. The idea behind this entity is to allow applications that share commonly used schemas to have information regarding the library that the developer can use to integrate with the platform directly instead of having to manually implement the client themselves. An application can have any number of connectors and a connector is compose of 2 things: language : The language of the connector library. package_url : The URL of the connector library to download. Official Templates make use of connectors to allow you to quickly create applications that share a common use.","title":"Connector"},{"location":"reference/connector/#connector","text":"A connector is an easy way to include a pre-implemented data upload client in the information of an application. The idea behind this entity is to allow applications that share commonly used schemas to have information regarding the library that the developer can use to integrate with the platform directly instead of having to manually implement the client themselves. An application can have any number of connectors and a connector is compose of 2 things: language : The language of the connector library. package_url : The URL of the connector library to download. Official Templates make use of connectors to allow you to quickly create applications that share a common use.","title":"Connector"},{"location":"reference/data-entries/","text":"Data Entries \u00b6 Every application exposes an endpoint on the server to upload data entries. Data entries need to conform to the schema defined in the application. Depending on the schema defined, you may or may not upload incomplete data. This is useful in the case where your schema includes data properties for which you might not be able to upload data at the same time. For example, in the example seen in creating your first application , for the following schema: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number The property daily_server_count is supposed to be uploaded once every day on a cronjob basis. For the case of the rest of the properties, they're supposed to be uploaded whenever a user executes a command, and whether this command errors or not it may or may not include the command_error property. Given this case, there will be rows for each data entry where not everything will have a value. For more information on how to upload data, check out uploading data .","title":"Data Entries"},{"location":"reference/data-entries/#data-entries","text":"Every application exposes an endpoint on the server to upload data entries. Data entries need to conform to the schema defined in the application. Depending on the schema defined, you may or may not upload incomplete data. This is useful in the case where your schema includes data properties for which you might not be able to upload data at the same time. For example, in the example seen in creating your first application , for the following schema: type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number The property daily_server_count is supposed to be uploaded once every day on a cronjob basis. For the case of the rest of the properties, they're supposed to be uploaded whenever a user executes a command, and whether this command errors or not it may or may not include the command_error property. Given this case, there will be rows for each data entry where not everything will have a value. For more information on how to upload data, check out uploading data .","title":"Data Entries"},{"location":"reference/schema/","text":"Schema \u00b6 Every application has a schema that defines the structure of the data that will be uploaded. Apart from defining the structure of the data it is also used to validate it on upload. Schemas cannot be updated when an application is created. If you need to change the structure of the data for your application then you need to create a new application. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. A schema should be valid a JSON Schema written in YML. Every schema should be of type object and can have any type as properties. The server will validate data on upload with the flag additionalProperties set to false . If your schema contains this directive it will be overridden for the top level object. Some examples of valid schemas: type : object properties : name : type : string phone : type : number nested_data : type : object properties : title : type : string subtitle : type : string required : - title - subtitle arr_numbers : type : array items : type : number required : - name - phone Or, type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number","title":"Schema"},{"location":"reference/schema/#schema","text":"Every application has a schema that defines the structure of the data that will be uploaded. Apart from defining the structure of the data it is also used to validate it on upload. Schemas cannot be updated when an application is created. If you need to change the structure of the data for your application then you need to create a new application. This limitation comes from the fact that the schema defines the structure of the data and how it is visualized. If this changes, fields that are removed may no longer be visualized even though data still exists for those fields. A schema should be valid a JSON Schema written in YML. Every schema should be of type object and can have any type as properties. The server will validate data on upload with the flag additionalProperties set to false . If your schema contains this directive it will be overridden for the top level object. Some examples of valid schemas: type : object properties : name : type : string phone : type : number nested_data : type : object properties : title : type : string subtitle : type : string required : - title - subtitle arr_numbers : type : array items : type : number required : - name - phone Or, type : object properties : command_error : type : string command_executed : type : string command_author : type : string daily_server_count : type : number","title":"Schema"}]}